{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb02cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "beta_file_path = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/software/prscsx/res/data/\"\n",
    "output_path = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/software/prscsx/res/merged_data/\"\n",
    "\n",
    "def merge_beta_files(trait):\n",
    "    file_pattern = f\"{trait}_EAS_*_chr*.txt\"\n",
    "    output_file_path = f\"{output_path}{trait}_merged.txt\"\n",
    "    search_path = f\"{beta_file_path}{file_pattern}\"\n",
    "    file_list = sorted(glob.glob(search_path))\n",
    "    if not file_list:\n",
    "        print(f\"No files found for trait: {trait}\")\n",
    "        return\n",
    "    else:\n",
    "        print(f\"Found {len(file_list)} files for trait: {trait}\")\n",
    "        df_list = []\n",
    "        for f in file_list:\n",
    "            print(f\"Processing file: {f}\")\n",
    "            df = pd.read_csv(f, sep=r'\\s+', header=None)\n",
    "            df_list.append(df)\n",
    "        merged_df = pd.concat(df_list, ignore_index=True)\n",
    "        merged_df.to_csv(output_file_path, sep='\\t', header=False, index=False)\n",
    "\n",
    "trait_dict = {\n",
    "    'p48': 'waist',\n",
    "    'p50': 'height',\n",
    "    'p102': 'pulse',\n",
    "    'p4079': 'dbp',\n",
    "    'p4080': 'sbp',\n",
    "    'p20116': 'smoke',\n",
    "    'p20117': 'drink',\n",
    "    'p21001': 'bmi',\n",
    "    'p21002': 'weight',\n",
    "    'p30000': 'wbc',\n",
    "    'p30010': 'rbc',\n",
    "    'p30020':'hb',\n",
    "    'p30080': 'plt',\n",
    "    'p30120': 'lymph',\n",
    "    'p30130': 'mono',\n",
    "    'p30140': 'neut',\n",
    "    'p30150': 'eos',\n",
    "    'p30620': 'alt',\n",
    "    'p30650': 'ast',\n",
    "    'p30670': 'bun',\n",
    "    'p30690': 'cholesterol',\n",
    "    'p30700': 'creatinine',\n",
    "    'p30730': 'ggt',\n",
    "    'p30740': 'glucose',\n",
    "    'p30760': 'hdl',\n",
    "    'p30780': 'ldl',\n",
    "    'p30870': 'triglycerides',\n",
    "    'p30880': 'ua'\n",
    "}\n",
    "\n",
    "for trait, name in trait_dict.items():\n",
    "    trait_name = f'{name}_{trait}'\n",
    "    print(f\"Merging files for trait: {trait_name}\")\n",
    "    merge_beta_files(trait_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb40250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "beta_file_path = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/software/prscsx/res/merged_data/\"\n",
    "test_bfile_path = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/UKB/geno/Chinese/1_merged/merged\"\n",
    "plink_path = \"/data1/jiapl_group/lishuhua/software/general/plink\"\n",
    "output_path = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/software/prscsx/res/test_prs\"\n",
    "\n",
    "tasks = []\n",
    "# alt_p30620_EAS_pst_eff_a1_b0.5_phiauto_chr15.txt\n",
    "for beta_file in os.listdir(beta_file_path):\n",
    "    if beta_file.endswith(\".txt\"):\n",
    "        beta = os.path.join(beta_file_path, beta_file)\n",
    "        trait = beta_file.replace(\"_merged.txt\", \"\")\n",
    "        output = os.path.join(output_path, f\"{trait}_PRS\")\n",
    "        cmd = f\"{plink_path} --bfile {test_bfile_path} --score {beta} 2 4 6 no-mean-imputation --out {output}\"\n",
    "        tasks.append(cmd)\n",
    "\n",
    "pool = Pool(cpu_count()-1)\n",
    "pool.map(os.system, tasks)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6063275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "prs_file_path = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/software/prscsx/res/test_prs\"\n",
    "output_path = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/software/prscsx/res/test_prs_cleaned\"\n",
    "\n",
    "for prs_file in os.listdir(prs_file_path):\n",
    "    if prs_file.endswith(\"PRS.profile\"):\n",
    "        prs_data = open(os.path.join(prs_file_path, prs_file), \"r\")\n",
    "        prs_list = []\n",
    "        for line in prs_data:\n",
    "            line_arr = line.strip().split(\" \")\n",
    "            line_arr = [x for x in line_arr if x != \"\"]\n",
    "            prs_list.append(line_arr)\n",
    "        prs_data.close()\n",
    "        prs_df = pd.DataFrame(prs_list[1:], columns=prs_list[0])\n",
    "        trait = prs_file.replace(\"_PRS.profile\", \"\")\n",
    "        prs_df.to_csv(os.path.join(output_path, f\"{trait}_PRS.tsv\"), sep='\\t', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f48f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import (r2_score, mean_squared_error, roc_auc_score,\n",
    "average_precision_score, roc_curve)\n",
    "from sklearn.calibration import calibration_curve\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "cleaned_prs_path = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/software/prscsx/res/test_prs_cleaned\"\n",
    "covar_path = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/UKB/pheno/covar/covars_chinese_final.tsv\"\n",
    "output_path = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/software/prscsx/res/full_res\"\n",
    "figure_path = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/software/prscsx/res/full_res/figures\"\n",
    "pheno_dir = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/UKB/pheno/trait/Chinese\"\n",
    "\n",
    "n_bootstrap = 1000\n",
    "\n",
    "result_continus = []\n",
    "result_binary = []\n",
    "covar_cols = [\"FID\", \"IID\", \"age\", \"sex\"] + [f\"PC{i}\" for i in range(1, 11)]\n",
    "covars = pd.read_csv(covar_path, sep='\\t', usecols=covar_cols)\n",
    "base_covars = [\"age\", \"sex\"] + [f\"PC{i}\" for i in range(1, 11)]\n",
    "full_covars = base_covars + [\"SCORE\"]\n",
    "for prs_file in os.listdir(cleaned_prs_path):\n",
    "    trait = prs_file.split(\"_\")[1]\n",
    "    trait = trait + '_int'\n",
    "    for pheno_file in os.listdir(pheno_dir):\n",
    "        if pheno_file.startswith(trait) and pheno_file.endswith(\".txt\"):\n",
    "            pheno = pd.read_csv(os.path.join(pheno_dir, pheno_file), sep='\\t')\n",
    "            pheno.columns = [\"FID\", \"IID\", \"trait\"]\n",
    "            prs = pd.read_csv(os.path.join(cleaned_prs_path, prs_file), sep='\\t')\n",
    "            merged_data = pd.merge(pheno, prs, on=[\"FID\", \"IID\"], how=\"inner\")\n",
    "            merged_data = pd.merge(merged_data, covars, on=[\"FID\", \"IID\"], how=\"inner\")\n",
    "            # check the rows in merged_data and pheno\n",
    "            if merged_data.shape[0] == pheno.shape[0]:\n",
    "                print(f\"All rows matched for {pheno_file} and {prs_file}.\")\n",
    "            if trait == \"p20116_int\" or trait == \"p20117_int\":\n",
    "                # do the binary test\n",
    "                # Ensure binary trait is coded as 0/1\n",
    "                if not set(merged_data[\"trait\"].unique()).issubset({0, 1}):\n",
    "                    # Try to convert to 0/1 if possible\n",
    "                    unique_vals = sorted(merged_data[\"trait\"].unique())\n",
    "                    if len(unique_vals) == 2:\n",
    "                        merged_data[\"trait\"] = (merged_data[\"trait\"] == unique_vals[1]).astype(int)\n",
    "                    else:\n",
    "                        raise ValueError(f\"Trait column contains unexpected values: {unique_vals}\")\n",
    "\n",
    "                logit_model = sm.Logit(merged_data[\"trait\"], sm.add_constant(merged_data[full_covars])).fit(disp=0)\n",
    "                pred_prob = logit_model.predict(sm.add_constant(merged_data[full_covars]))\n",
    "\n",
    "                # Step1: calculate AUC\n",
    "                auc = roc_auc_score(merged_data[\"trait\"], pred_prob)\n",
    "                pr_auc = average_precision_score(merged_data[\"trait\"], pred_prob)\n",
    "\n",
    "                # Step2: calculate OR per 1-SD\n",
    "                merged_data[\"prs_scaled\"] = (merged_data[\"SCORE\"] - merged_data[\"SCORE\"].mean()) / merged_data[\"SCORE\"].std()\n",
    "                X_full_scaled = sm.add_constant(merged_data[full_covars + [\"prs_scaled\"]])\n",
    "                logit_model_scaled = sm.Logit(merged_data[\"trait\"], X_full_scaled).fit(disp=0)\n",
    "                or_per_sd = np.exp(logit_model_scaled.params[\"prs_scaled\"])\n",
    "\n",
    "                # Step3: calculate quantile stratification\n",
    "                merged_data['prs_quintile'] = pd.qcut(merged_data['SCORE'], 5, labels=False, duplicates='drop')\n",
    "                # 使用中间组(第2组, index=2)作为参照\n",
    "                reference_quintile = 2\n",
    "                or_quintiles = {}\n",
    "\n",
    "                for q in range(5):\n",
    "                    if q == reference_quintile:\n",
    "                        or_quintiles[f'Quintile {q+1}'] = 1.0\n",
    "                        continue\n",
    "\n",
    "                    # 仅比较当前分位数和参照分位数\n",
    "                    temp_df = merged_data[merged_data['prs_quintile'].isin([q, reference_quintile])].copy()\n",
    "                    temp_df['is_current_quintile'] = (temp_df['prs_quintile'] == q).astype(int)\n",
    "    \n",
    "                    X_quintile = sm.add_constant(temp_df[['is_current_quintile'] + base_covars])\n",
    "                    model_q = sm.Logit(temp_df[\"trait\"], X_quintile).fit(disp=0)\n",
    "                    or_quintiles[f'Quintile {q+1}'] = np.exp(model_q.params['is_current_quintile'])\n",
    "\n",
    "                print(\"2.3.1 Quintile Odds Ratios (OR): (Reference: Quintile 3)\")\n",
    "                for q_name, or_val in or_quintiles.items():\n",
    "                    print(f\" {q_name}: {or_val:.2f}\")\n",
    "\n",
    "                # 可视化分位数OR\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                sns.barplot(x=list(or_quintiles.keys()), y=list(or_quintiles.values()), color='coral')\n",
    "                plt.axhline(1.0, color='black', linestyle='--')\n",
    "                plt.title('Disease Risk Odds Ratio (OR) vs. PRS Quintiles', fontsize=16)\n",
    "                plt.xlabel('PRS Quintile', fontsize=12)\n",
    "                plt.ylabel('Odds Ratio (vs. Quintile 3)', fontsize=12)\n",
    "                plt.savefig(os.path.join(figure_path, f\"{trait}_prs_quintile_or.png\"))\n",
    "\n",
    "                # --- 2.4 校准度 (Calibration) ---\n",
    "                prob_true, prob_pred = calibration_curve(merged_data[\"trait\"], pred_prob, n_bins=10, strategy='uniform')\n",
    "\n",
    "                plt.figure(figsize=(8, 8))\n",
    "                plt.plot(prob_pred, prob_true, marker='o', linewidth=1, label='Calibration Curve')\n",
    "                plt.plot([0, 1], [0, 1], linestyle='--', color='black', label='Perfect Calibration')\n",
    "                plt.xlabel('Mean Predicted Probability', fontsize=12)\n",
    "                plt.ylabel('Fraction of Positives (True Probability)', fontsize=12)\n",
    "                plt.title('Calibration Plot', fontsize=16)\n",
    "                plt.legend()\n",
    "                plt.savefig(os.path.join(figure_path, f\"{trait}_calibration_curve.png\"))\n",
    "                plt.close()\n",
    "\n",
    "                result_binary.append({\n",
    "                    \"trait\": trait,\n",
    "                    \"auc\": auc,\n",
    "                    \"pr_auc\": pr_auc,\n",
    "                    \"or_per_sd\": or_per_sd,\n",
    "                    \"quantile_or\": or_quintiles\n",
    "                })\n",
    "\n",
    "            else:\n",
    "                # do the continuous test\n",
    "                # Step1: caculate incremental R2\n",
    "                model_base = sm.OLS(merged_data[\"trait\"], sm.add_constant(merged_data[base_covars])).fit()\n",
    "                model_full = sm.OLS(merged_data[\"trait\"], sm.add_constant(merged_data[full_covars])).fit()\n",
    "                r2_incremental = model_full.rsquared - model_base.rsquared\n",
    "\n",
    "                # Step2: calculate R2, RMSE, Pearson's r\n",
    "                pheno_residuals = model_base.resid\n",
    "                corr, p_val = pearsonr(merged_data[\"SCORE\"], pheno_residuals)\n",
    "\n",
    "                # Step3: calculate RMSE\n",
    "                prediction_full = model_full.predict(sm.add_constant(merged_data[full_covars]))\n",
    "                rmse = np.sqrt(mean_squared_error(merged_data[\"trait\"], prediction_full))\n",
    "\n",
    "                # Step 4: calculate quantile stratification\n",
    "                merged_data['quantile'] = pd.qcut(merged_data['SCORE'], 5, labels=False, duplicates='drop')\n",
    "                quantile_means = merged_data.groupby('quantile')['trait'].mean()\n",
    "                top_decile_mean = quantile_means.iloc[-1]\n",
    "                bottom_decile_mean = quantile_means.iloc[0]\n",
    "\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                sns.barplot(x=quantile_means.index + 1, y=quantile_means.values, color='skyblue')\n",
    "                plt.title('Quantitative Trait Mean vs. PRS Deciles', fontsize=16)\n",
    "                plt.xlabel('PRS Decile (1=Lowest, 5=Highest)', fontsize=12)\n",
    "                plt.ylabel(f'Mean {trait}', fontsize=12)\n",
    "                plt.savefig(os.path.join(figure_path, f\"{trait}_prs_decile_mean.png\"))\n",
    "                plt.close()\n",
    "\n",
    "                result_continus.append({\n",
    "                    \"trait\": trait,\n",
    "                    \"r2_incremental\": r2_incremental,\n",
    "                    \"r2\": model_full.rsquared,\n",
    "                    \"rmse\": rmse,\n",
    "                    \"pearson_r\": corr,\n",
    "                    \"top_decile_mean\": top_decile_mean,\n",
    "                    \"bottom_decile_mean\": bottom_decile_mean\n",
    "                })\n",
    "result_continus_df = pd.DataFrame(result_continus)\n",
    "result_continus_df.to_csv(os.path.join(output_path, \"results_continuous.tsv\"), sep='\\t', index=False)\n",
    "result_binary_df = pd.DataFrame(result_binary)\n",
    "result_binary_df.to_csv(os.path.join(output_path, \"results_binary.tsv\"), sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
