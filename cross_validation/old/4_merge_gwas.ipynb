{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1b3d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Continuous: train_chr1.Pheno.glm.linear\n",
    "# Binary: train_chr14.Pheno.glm.logistic\n",
    "\n",
    "# UKBB EUR SNP counts list\n",
    "count_list = {\n",
    "    '1': 598771,\n",
    "    '2': 655977,\n",
    "    '3': 557352,\n",
    "    '4': 577011,\n",
    "    '5': 508272,\n",
    "    '6': 535480,\n",
    "    '7': 455119,\n",
    "    '8': 432227,\n",
    "    '9': 329660,\n",
    "    '10': 397640,\n",
    "    '11': 392275,\n",
    "    '12': 375538,\n",
    "    '13': 289173,\n",
    "    '14': 254513,\n",
    "    '15': 213428,\n",
    "    '16': 226288,\n",
    "    '17': 197587,\n",
    "    '18': 219460,\n",
    "    '19': 165968,\n",
    "    '20': 168391,\n",
    "    '21': 104766,\n",
    "    '22': 99925\n",
    "}\n",
    "\n",
    "FILE_PREFIX = \"train\"\n",
    "FILE_SUFFIX = \"_chr\"\n",
    "FILE_EXTENSION = \".Pheno.glm.linear\"\n",
    "FILE_EXTENSION2 = \".Pheno.glm.logistic\"\n",
    "\n",
    "def merge_gwas_files(\n",
    "    GWAS_DIR,\n",
    "    OUTPUT_DIR,\n",
    "    TRAIT,\n",
    "    check_files=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Main function to perform the GWAS file checking and merging process.\n",
    "    \"\"\"\n",
    "    # 确保输出目录存在\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"==================================================\")\n",
    "    print(\"Starting GWAS file merge process.\")\n",
    "    print(f\"GWAS Directory:   {GWAS_DIR}\")\n",
    "    print(f\"Output Directory: {OUTPUT_DIR}\")\n",
    "    print(\"==================================================\")\n",
    "\n",
    "    # 对性状列表中的每一个性状进行循环\n",
    "    pheno = TRAIT\n",
    "    print(f\"\\n--- Processing phenotype: [ {pheno} ] ---\")\n",
    "    all_files_exist = True\n",
    "    files_to_merge = []\n",
    "    # **安全检查**: 检查1到22号染色体的文件是否都存在\n",
    "    print(\"Checking for presence of all 22 chromosome files...\")\n",
    "    for i in range(1, 23):\n",
    "        # 根据规则确定当前性状的文件扩展名\n",
    "        if pheno in [\"drink\", \"smoke\"]:\n",
    "            final_extension = FILE_EXTENSION2\n",
    "        else:\n",
    "            final_extension = FILE_EXTENSION\n",
    "        # 根据您定义的规则拼凑出预期的文件名\n",
    "        # 示例: p48_int_chr1.p48_int.glm.linear\n",
    "        filename = f\"{FILE_PREFIX}{FILE_SUFFIX}{i}{final_extension}\"\n",
    "        expected_file = GWAS_DIR / filename\n",
    "        # 使用 .is_file() 来判断文件是否存在且是一个文件\n",
    "        if not expected_file.is_file():\n",
    "            print(f\"  [ERROR] Missing file: {expected_file}\", file=sys.stderr)\n",
    "            all_files_exist = False\n",
    "            break  # 只要有一个文件缺失，就没必要再检查了\n",
    "        file_rows = count_list.get(str(i), 0)\n",
    "        # calculate the rows in the file\n",
    "        with open(expected_file, 'r') as f:\n",
    "            #  remove the header line\n",
    "            actual_rows = sum(1 for line in f) - 1  # 减去表头行\n",
    "        if actual_rows != file_rows:\n",
    "            print(f\"  [ERROR] File {expected_file} has {actual_rows} rows, expected {file_rows} rows.\", file=sys.stderr)\n",
    "            all_files_exist = False\n",
    "            break\n",
    "        \n",
    "        files_to_merge.append(expected_file)\n",
    "    # **条件合并**: 只有在所有文件都存在的情况下才执行合并操作\n",
    "    if all_files_exist:\n",
    "        print(\"  All 22 files found. Proceeding with merge.\")\n",
    "        # 定义最终的输出文件名\n",
    "        # 原始脚本都输出为 .glm.linear，这里保持一致\n",
    "        output_file = OUTPUT_DIR / f\"{pheno}.merged.glm.linear\"\n",
    "        try:\n",
    "            with open(output_file, 'w') as outfile:\n",
    "                # 遍历已确认存在且顺序正确的文件列表\n",
    "                for i, filepath in enumerate(files_to_merge):\n",
    "                    with open(filepath, 'r') as infile:\n",
    "                        # 第一个文件 (i=0)，完整写入 (包括表头)\n",
    "                        if i == 0:\n",
    "                            for line in infile:\n",
    "                                outfile.write(line)\n",
    "                        # 后续文件，跳过表头\n",
    "                        else:\n",
    "                            next(infile)  # 跳过第一行\n",
    "                            for line in infile:\n",
    "                                outfile.write(line)\n",
    "            \n",
    "            # 检查输出文件是否成功生成且不为空\n",
    "            if output_file.exists() and output_file.stat().st_size > 0:\n",
    "                print(\"  Successfully merged. Output file created:\")\n",
    "                check_files = True\n",
    "                print(f\"  -> {output_file}\")\n",
    "            else:\n",
    "                print(f\"  [ERROR] Merging failed for phenotype [ {pheno} ]. Output file is empty.\", file=sys.stderr)\n",
    "        except IOError as e:\n",
    "            print(f\"  [ERROR] An I/O error occurred during merge: {e}\", file=sys.stderr)\n",
    "    else:\n",
    "        # 如果文件不齐全，则打印跳过信息\n",
    "        print(f\"  Skipping merge for phenotype [ {pheno} ] due to missing files.\")\n",
    "    return check_files\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    meta_df = pd.DataFrame()\n",
    "    TRAIT_LIST = ['height', 'pulse', 'dbp', 'sbp', 'smoke', 'drink', 'bmi', 'waist', 'wbc', 'rbc', 'hb', 'plt', 'lymph', 'mono', 'neut', 'eos', 'alt', 'ast', 'bun', 'cholesterol', 'creatinine', 'glucose', 'ggt', 'hdl', 'ldl', 'triglycerides', 'ua']\n",
    "    for trait in TRAIT_LIST:\n",
    "        for i in range(1, 11):\n",
    "            print(f\"\\nProcessing trait: {trait}, iteration: {i}\")\n",
    "            # 定义GWAS目录和输出目录\n",
    "            GWAS_DIR = Path(f\"/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/Cross_Validation/UKB_EUR/{trait}/group_{i}/gwas/\")\n",
    "            OUTPUT_DIR = Path(f\"/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/Cross_Validation/UKB_EUR/{trait}/group_{i}/merged_gwas/\")\n",
    "            if os.path.exists(f'{OUTPUT_DIR}/{trait}.merged.glm.linear'):\n",
    "                print(f\"  Output file already exists for trait {trait}, group {i}. Skipping merge.\")\n",
    "                check_files = True\n",
    "            else:\n",
    "                check_files = merge_gwas_files(GWAS_DIR, OUTPUT_DIR, trait)\n",
    "            if check_files:\n",
    "                # 如果文件检查通过，添加到meta_df\n",
    "                meta_df = pd.concat([meta_df, pd.DataFrame({'trait': [trait], 'group': [i], 'status': ['merged']})], ignore_index=True)\n",
    "            else:\n",
    "                # 如果文件检查不通过，添加到meta_df\n",
    "                meta_df = pd.concat([meta_df, pd.DataFrame({'trait': [trait], 'group': [i], 'status': ['skipped']})], ignore_index=True)\n",
    "    # 保存meta_df到CSV文件\n",
    "    meta_output_path = Path(\"/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/Cross_Validation/meta_gwas_ukb.tsv\")\n",
    "    meta_df.to_csv(meta_output_path, index=False, header=True, sep='\\t')\n",
    "\n",
    "print(\"\\n==================================================\")\n",
    "print(\"Process finished.\")\n",
    "print(\"==================================================\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
