{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284b49b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EAS clump res demo: /data1/jiapl_group/lishuhua/project/PRS_benchmark/software/ct/res/clumped/waist/group_1/r2_0.1_w_500/res.clumps\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "trait_list = ['waist', 'height', 'pulse', 'dbp', 'sbp', 'smoke', 'drink', 'bmi', 'wbc', 'rbc', 'hb', 'plt', 'lymph', 'mono', 'neut', 'eos', 'alt', 'ast', 'bun', 'cholesterol', 'creatinine', 'glucose', 'ggt', 'hdl', 'ldl', 'triglycerides', 'ua']\n",
    "eas_base_dir = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/Cross_Validation/CAS/\"\n",
    "clump_res_dir = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/software/ct/res/clumped/\"\n",
    "output_dir = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/software/ct/res/filtered/\"\n",
    "\n",
    "p_val_list = [5e-8, 5e-7, 5e-6, 5e-5, 5e-4, 5e-3, 5e-2, 0.5, 1]\n",
    "\n",
    "for trait in trait_list:\n",
    "    for group in range(1, 11):\n",
    "        gwas_file = os.path.join(eas_base_dir, f\"{trait}/group_{group}/gwas/train.Pheno.glm.linear\")\n",
    "        if not os.path.exists(gwas_file):\n",
    "            print(f\"File {gwas_file} does not exist. Skipping.\")\n",
    "            continue\n",
    "        gwas_df = pd.read_csv(gwas_file, sep=\"\\t\")\n",
    "        for p_val in p_val_list:\n",
    "            print(f\"Processing trait {trait}, group {group}, p_val {p_val}\")\n",
    "            clump_file = os.path.join(clump_res_dir, f\"{trait}/group_{group}/r2_0.1_w_500/res.clumps\")\n",
    "            if not os.path.exists(clump_file):\n",
    "                print(f\"File {clump_file} does not exist. Skipping.\")\n",
    "                continue\n",
    "            df = pd.read_csv(clump_file, sep=\"\\t\")\n",
    "            df_filtered = df[df['P'] <= p_val]\n",
    "            df_filtered = df_filtered[[\"#CHROM\", \"POS\", \"ID\"]]\n",
    "            res = pd.merge(df_filtered, gwas_df, on=[\"#CHROM\", \"POS\", \"ID\"], how='inner')\n",
    "            if res.shape[0] == 0:\n",
    "                print(f\"No SNPs passed the p-value threshold {p_val} for trait {trait}, group {group}. Skipping.\")\n",
    "                continue\n",
    "            output_subdir = os.path.join(output_dir, f\"{trait}/EAS/group_{group}/\")\n",
    "            os.makedirs(output_subdir, exist_ok=True)\n",
    "            output_file = os.path.join(output_subdir, f\"pval_{p_val}.clumped\")\n",
    "            res.to_csv(output_file, sep=\"\\t\", index=False, header=True)\n",
    "            print(f\"Saved filtered clump file to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e453ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate PRS based on the filtered clump files on EAS test data\n",
    "# test data demo: /data1/jiapl_group/lishuhua/project/PRS_benchmark/software/ct/test/alt/group_1/test\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "trait_list = ['waist', 'height', 'pulse', 'dbp', 'sbp', 'smoke', 'drink', 'bmi', 'wbc', 'rbc', 'hb', 'plt', 'lymph', 'mono', 'neut', 'eos', 'alt', 'ast', 'bun', 'cholesterol', 'creatinine', 'glucose', 'ggt', 'hdl', 'ldl', 'triglycerides', 'ua']\n",
    "clump_res_dir = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/software/ct/res/filtered/\"\n",
    "eas_base_dir = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/Cross_Validation/CAS/\"\n",
    "ct_base_dir = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/software/ct/\"\n",
    "output_dir = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/software/ct/res/test_in_sample/\"\n",
    "\n",
    "p_val_list = [5e-8, 5e-7, 5e-6, 5e-5, 5e-4, 5e-3, 5e-2, 0.5, 1]\n",
    "\n",
    "for trait in trait_list:\n",
    "    for group in range(1, 11):\n",
    "        test_bfile = os.path.join(ct_base_dir, f\"test/{trait}/group_{group}/combine\")\n",
    "        if not os.path.exists(test_bfile + \".bed\"):\n",
    "            print(f\"File {test_bfile}.bed does not exist. Skipping.\")\n",
    "            continue\n",
    "        for p_val in p_val_list:\n",
    "            print(f\"Processing trait {trait}, group {group}, p_val {p_val}\")\n",
    "            clump_file = os.path.join(clump_res_dir, f\"{trait}/EAS/group_{group}/pval_{p_val}.clumped\")\n",
    "            if not os.path.exists(clump_file):\n",
    "                print(f\"File {clump_file} does not exist. Skipping.\")\n",
    "                continue\n",
    "            output_subdir = os.path.join(output_dir, f\"EAS/{trait}/group_{group}/\")\n",
    "            os.makedirs(output_subdir, exist_ok=True)\n",
    "            output_prefix = os.path.join(output_subdir, f\"pval_{p_val}\")\n",
    "            prs_command = f\"plink2 --bfile {test_bfile} --score {clump_file} 3 5 12 header no-mean-imputation --out {output_prefix}\"\n",
    "            os.system(prs_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0354bae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings that may arise from certain fits in statsmodels\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "warnings.simplefilter('ignore', ConvergenceWarning)\n",
    "\n",
    "# --- 1. Metric Calculation Functions ---\n",
    "\n",
    "def calculate_continuous_metrics(df, base_covars, full_covars):\n",
    "    \"\"\"Calculates all performance metrics for a continuous trait for a given dataset (df).\"\"\"\n",
    "    # Incremental RÂ²\n",
    "    model_base = sm.OLS(df[\"trait\"], sm.add_constant(df[base_covars])).fit()\n",
    "    model_full = sm.OLS(df[\"trait\"], sm.add_constant(df[full_covars])).fit()\n",
    "    r2_incremental = model_full.rsquared - model_base.rsquared\n",
    "\n",
    "    # Pearson correlation coefficient (SCORE vs. phenotype residuals)\n",
    "    pheno_residuals = model_base.resid\n",
    "    corr, _ = pearsonr(df[\"SCORE\"], pheno_residuals)\n",
    "\n",
    "    # RMSE\n",
    "    prediction_full = model_full.predict(sm.add_constant(df[full_covars]))\n",
    "    rmse = np.sqrt(mean_squared_error(df[\"trait\"], prediction_full))\n",
    "    nrmse_mean = rmse / df[\"trait\"].mean() if df[\"trait\"].mean() != 0 else np.nan\n",
    "    nrmse_range = rmse / (df[\"trait\"].max() - df[\"trait\"].min()) if (df[\"trait\"].max() - df[\"trait\"].min()) != 0 else np.nan\n",
    "    nrmse_std = rmse / df[\"trait\"].std() if df[\"trait\"].std() != 0 else np.nan\n",
    "\n",
    "    # Quantile means\n",
    "    df['quantile'] = pd.qcut(df['SCORE'], 5, labels=False, duplicates='drop')\n",
    "    quantile_means = df.groupby('quantile')['trait'].mean()\n",
    "    \n",
    "    return {\n",
    "        \"r2_incremental\": r2_incremental,\n",
    "        \"r2_full\": model_full.rsquared,\n",
    "        \"rmse\": rmse,\n",
    "        \"nrmse_mean\": nrmse_mean,\n",
    "        \"nrmse_range\": nrmse_range,\n",
    "        \"nrmse_std\": nrmse_std,\n",
    "        \"pearson_r\": corr,\n",
    "        \"top_quintile_mean\": quantile_means.iloc[-1] if not quantile_means.empty else np.nan,\n",
    "        \"bottom_quintile_mean\": quantile_means.iloc[0] if not quantile_means.empty else np.nan\n",
    "    }\n",
    "\n",
    "def calculate_binary_metrics(df, base_covars, full_covars):\n",
    "    \"\"\"Calculates all performance metrics for a binary trait for a given dataset (df).\"\"\"\n",
    "    # AUC and PR-AUC\n",
    "    logit_model = sm.Logit(df[\"trait\"], sm.add_constant(df[full_covars])).fit(disp=0)\n",
    "    pred_prob = logit_model.predict(sm.add_constant(df[full_covars]))\n",
    "    auc = roc_auc_score(df[\"trait\"], pred_prob)\n",
    "    pr_auc = average_precision_score(df[\"trait\"], pred_prob)\n",
    "\n",
    "    # OR per 1-SD\n",
    "    df[\"prs_scaled\"] = (df[\"SCORE\"] - df[\"SCORE\"].mean()) / df[\"SCORE\"].std()\n",
    "    logit_model_scaled = sm.Logit(df[\"trait\"], sm.add_constant(df[base_covars + [\"prs_scaled\"]])).fit(disp=0)\n",
    "    or_per_sd = np.exp(logit_model_scaled.params[\"prs_scaled\"])\n",
    "\n",
    "    # Quantile OR\n",
    "    df['prs_quintile'] = pd.qcut(df['SCORE'], 5, labels=False, duplicates='drop')\n",
    "    reference_quintile = 2 # Middle quintile\n",
    "    or_quintiles = {}\n",
    "    for q in range(5):\n",
    "        if q == reference_quintile:\n",
    "            or_quintiles[f'OR_Quintile_{q+1}'] = 1.0\n",
    "            continue\n",
    "        \n",
    "        # Check if both current and reference quintiles exist in the data\n",
    "        if not df['prs_quintile'].isin([q, reference_quintile]).all():\n",
    "             or_quintiles[f'OR_Quintile_{q+1}'] = np.nan\n",
    "             continue\n",
    "             \n",
    "        temp_df = df[df['prs_quintile'].isin([q, reference_quintile])].copy()\n",
    "        \n",
    "        # Check for sufficient data in both groups for stable model fitting\n",
    "        if temp_df['trait'].nunique() < 2 or temp_df['prs_quintile'].nunique() < 2:\n",
    "            or_quintiles[f'OR_Quintile_{q+1}'] = np.nan\n",
    "            continue\n",
    "            \n",
    "        temp_df['is_current_quintile'] = (temp_df['prs_quintile'] == q).astype(int)\n",
    "        X_quintile = sm.add_constant(temp_df[['is_current_quintile'] + base_covars])\n",
    "        try:\n",
    "            model_q = sm.Logit(temp_df[\"trait\"], X_quintile).fit(disp=0)\n",
    "            or_quintiles[f'OR_Quintile_{q+1}'] = np.exp(model_q.params['is_current_quintile'])\n",
    "        except Exception:\n",
    "            or_quintiles[f'OR_Quintile_{q+1}'] = np.nan\n",
    "            \n",
    "    results = {\n",
    "        \"auc\": auc,\n",
    "        \"pr_auc\": pr_auc,\n",
    "        \"or_per_sd\": or_per_sd,\n",
    "    }\n",
    "    results.update(or_quintiles) # Merge quantile ORs into the results dictionary\n",
    "    return results\n",
    "\n",
    "# --- 2. Main Execution Flow ---\n",
    "\n",
    "def main():\n",
    "    # --- Parameter Settings ---\n",
    "    # !! Note: Please modify the variables below according to your actual paths !!\n",
    "    cleaned_prs_path = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/software/ct/res/test_in_sample/\"\n",
    "    covar_path = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/CAS/pheno/covariates.txt\"\n",
    "    output_dir = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/software/ct/res/test_in_sample/\"\n",
    "    pheno_dir = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/Cross_Validation/CAS/\"\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # --- Initialization ---\n",
    "    final_results_continuous = []\n",
    "    covar_cols = [\"FID\", \"IID\", \"age\", \"sex\"] + [f\"PC{i}\" for i in range(1, 11)]\n",
    "    base_covars = [\"age\", \"sex\"] + [f\"PC{i}\" for i in range(1, 11)]\n",
    "    full_covars = base_covars + [\"SCORE\"]\n",
    "\n",
    "    # --- Data Loading and Processing ---\n",
    "    covars = pd.read_csv(covar_path, sep='\\t', usecols=covar_cols)\n",
    "\n",
    "    trait_dict = {\n",
    "        'p48': 'waist',\n",
    "        'p50': 'height',\n",
    "        'p102': 'pulse',\n",
    "        'p4079': 'dbp',\n",
    "        'p4080': 'sbp',\n",
    "        'p20116': 'smoke',\n",
    "        'p20117': 'drink',\n",
    "        'p21001': 'bmi',\n",
    "        'p30000': 'wbc',\n",
    "        'p30010': 'rbc',\n",
    "        'p30020':'hb',\n",
    "        'p30080': 'plt',\n",
    "        'p30120': 'lymph',\n",
    "        'p30130': 'mono',\n",
    "        'p30140': 'neut',\n",
    "        'p30150': 'eos',\n",
    "        'p30620': 'alt',\n",
    "        'p30650': 'ast',\n",
    "        'p30670': 'bun',\n",
    "        'p30690': 'cholesterol',\n",
    "        'p30700': 'creatinine',\n",
    "        'p30730': 'ggt',\n",
    "        'p30740': 'glucose',\n",
    "        'p30760': 'hdl',\n",
    "        'p30780': 'ldl',\n",
    "        'p30870': 'triglycerides',\n",
    "        'p30880': 'ua'\n",
    "    }\n",
    "    p_val_list = [5e-8, 5e-7, 5e-6, 5e-5, 5e-4, 5e-3, 5e-2, 0.5, 1]\n",
    "    # /data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/Cross_Validation/CAS/alt/group_1/pheno/test.txt\n",
    "\n",
    "    for trait, name in trait_dict.items():\n",
    "        for i in range(1, 11):\n",
    "            for p_val in p_val_list:\n",
    "                trait_prs_path = os.path.join(cleaned_prs_path, f\"EAS/{name}/group_{i}/pval_{p_val}.sscore\")\n",
    "                if not os.path.exists(trait_prs_path):\n",
    "                    print(f\"Warning: PRS file for trait {name} group {i} not found. Skipping.\")\n",
    "                    continue\n",
    "                print(f\"\\nProcessing Trait: {name}, Group: {i}, P-value: {p_val}\")\n",
    "                if trait == 'p20116' or trait == 'p20117':\n",
    "                    trait_id_from_prs = f\"{trait}_int\"\n",
    "                else:\n",
    "                    trait_id_from_prs = f\"{trait}_raw\"\n",
    "                pheno_path = os.path.join(pheno_dir, f\"{name}/group_{i}/pheno/test_pheno.txt\")\n",
    "                pheno = pd.read_csv(pheno_path, sep='\\t')\n",
    "                pheno.columns = [\"FID\", \"IID\", \"trait\"]\n",
    "                print(pheno.head())\n",
    "                prs = pd.read_csv(trait_prs_path, sep='\\t')\n",
    "                prs.rename(columns={\"#FID\": \"FID\"}, inplace=True)\n",
    "                prs.rename(columns={\"SCORE1_AVG\": \"SCORE\"}, inplace=True)\n",
    "\n",
    "                for df in [pheno, prs, covars]:\n",
    "                    df[\"FID\"] = df[\"FID\"].astype(str)\n",
    "                    df[\"IID\"] = df[\"IID\"].astype(str)\n",
    "                merged_data = pd.merge(pheno, covars, on=[\"FID\", \"IID\"], how=\"inner\")\n",
    "                merged_data = pd.merge(merged_data, prs, on=[\"FID\", \"IID\"], how=\"inner\")\n",
    "                print(merged_data.head())\n",
    "                # Defensive data cleaning and type conversion\n",
    "                numeric_cols = [\"trait\", \"SCORE\", \"age\", \"sex\"] + [f\"PC{i}\" for i in range(1, 11)]\n",
    "                for col in numeric_cols:\n",
    "                    if col in merged_data.columns:\n",
    "                        merged_data[col] = pd.to_numeric(merged_data[col], errors='coerce')\n",
    "\n",
    "                original_rows = len(merged_data)\n",
    "                merged_data.dropna(subset=numeric_cols, inplace=True)\n",
    "                new_rows = len(merged_data)\n",
    "                if original_rows > new_rows:\n",
    "                    print(f\"--> Warning: Dropped {original_rows - new_rows} rows due to non-numeric data or NaNs in trait {trait_id_from_prs}.\")\n",
    "                if new_rows == 0:\n",
    "                    print(f\"--> Error: No valid samples remained for trait {trait_id_from_prs} after cleaning. Skipping.\")\n",
    "                    continue\n",
    "                print(f\"Data merged and cleaned for trait {trait_id_from_prs}. Total samples: {len(merged_data)}\")\n",
    "\n",
    "                if trait_id_from_prs in [\"p20116_int\", \"p20117_int\"]:\n",
    "                    # Binary trait analysis\n",
    "                    # Ensure binary trait is 0/1 coded\n",
    "                    unique_vals = sorted(merged_data[\"trait\"].unique())\n",
    "                    if not set(unique_vals).issubset({0, 1}):\n",
    "                        if len(unique_vals) == 2:\n",
    "                            print(f\"Converting binary trait from {unique_vals} to 0/1.\")\n",
    "                            merged_data[\"trait\"] = (merged_data[\"trait\"] == unique_vals[1]).astype(int)\n",
    "                        else:\n",
    "                            print(f\"Error: Binary trait column for {trait_id_from_prs} contains unexpected values: {unique_vals}. Skipping.\")\n",
    "                            continue\n",
    "                        \n",
    "                    # Direct calculation of metrics\n",
    "                    analysis_report = calculate_continuous_metrics(merged_data, base_covars, full_covars)\n",
    "                    analysis_report['trait'] = trait_id_from_prs\n",
    "                    analysis_report['p_val_threshold'] = p_val\n",
    "                    final_results_continuous.append(analysis_report)\n",
    "                else:\n",
    "                    # Continuous trait analysis\n",
    "                    # Direct calculation of metrics\n",
    "                    analysis_report = calculate_continuous_metrics(merged_data, base_covars, full_covars)\n",
    "                    analysis_report['trait'] = trait_id_from_prs\n",
    "                    analysis_report['p_val_threshold'] = p_val\n",
    "                    final_results_continuous.append(analysis_report)\n",
    "\n",
    "        # --- 3. Save Final Results ---\n",
    "        if final_results_continuous:\n",
    "            continuous_df = pd.DataFrame(final_results_continuous)\n",
    "            # Reorder columns to have 'trait' first\n",
    "            cols = ['trait'] + [col for col in continuous_df.columns if col != 'trait']\n",
    "            continuous_df = continuous_df[cols]\n",
    "            continuous_df.to_csv(os.path.join(output_dir, \"EAS_in_sample_metrics.csv\"), index=False)\n",
    "            print(\"\\nContinuous trait results saved to EAS_in_sample_metrics.csv\")\n",
    "            print(continuous_df)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb82ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trait</th>\n",
       "      <th>p_val_threshold</th>\n",
       "      <th>average_incremental_r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>p30620_raw</td>\n",
       "      <td>5.000000e-06</td>\n",
       "      <td>5.615636e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>p30620_raw</td>\n",
       "      <td>5.000000e-02</td>\n",
       "      <td>5.498912e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>p30620_raw</td>\n",
       "      <td>5.000000e-04</td>\n",
       "      <td>5.405407e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>p30620_raw</td>\n",
       "      <td>5.000000e-05</td>\n",
       "      <td>4.948829e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>p30620_raw</td>\n",
       "      <td>5.000000e-03</td>\n",
       "      <td>4.800611e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>p30620_raw</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.208825e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>p30620_raw</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>2.872917e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>p30620_raw</td>\n",
       "      <td>5.000000e-07</td>\n",
       "      <td>3.557112e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          trait  p_val_threshold  average_incremental_r2\n",
       "96   p30620_raw     5.000000e-06            5.615636e-03\n",
       "100  p30620_raw     5.000000e-02            5.498912e-03\n",
       "98   p30620_raw     5.000000e-04            5.405407e-03\n",
       "97   p30620_raw     5.000000e-05            4.948829e-03\n",
       "99   p30620_raw     5.000000e-03            4.800611e-03\n",
       "102  p30620_raw     1.000000e+00            3.208825e-03\n",
       "101  p30620_raw     5.000000e-01            2.872917e-03\n",
       "95   p30620_raw     5.000000e-07            3.557112e-07"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# r2_data_path = \"../../../../PRS_benchmark/data/result/real_data/ct/EAS_in_sample_metrics.csv\"\n",
    "r2_data_path = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/software/ct/res/test_in_sample/EAS_in_sample_metrics.csv\"\n",
    "# eas_bfile_path = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/CAS/geno/CAS_final/CAS_merged_qc_final\"\n",
    "eas_bfile_path = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/software/tlprs/reference/EAS_1kg/1000G.EAS.QC.hm3.ind\"\n",
    "full_eas_gwas_dir = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/CAS/gwas/gwas/\"\n",
    "res_df = []\n",
    "output_base_dir = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/software/ct/res/full_model/\"\n",
    "\n",
    "trait_dict = {\n",
    "        'p48': 'waist',\n",
    "        'p50': 'height',\n",
    "        'p102': 'pulse',\n",
    "        'p4079': 'dbp',\n",
    "        'p4080': 'sbp',\n",
    "        'p20116': 'smoke',\n",
    "        'p20117': 'drink',\n",
    "        'p21001': 'bmi',\n",
    "        'p30000': 'wbc',\n",
    "        'p30010': 'rbc',\n",
    "        'p30020':'hb',\n",
    "        'p30080': 'plt',\n",
    "        'p30120': 'lymph',\n",
    "        'p30130': 'mono',\n",
    "        'p30140': 'neut',\n",
    "        'p30150': 'eos',\n",
    "        'p30620': 'alt',\n",
    "        'p30650': 'ast',\n",
    "        'p30670': 'bun',\n",
    "        'p30690': 'cholesterol',\n",
    "        'p30700': 'creatinine',\n",
    "        'p30730': 'ggt',\n",
    "        'p30740': 'glucose',\n",
    "        'p30760': 'hdl',\n",
    "        'p30780': 'ldl',\n",
    "        'p30870': 'triglycerides',\n",
    "        'p30880': 'ua'\n",
    "    }\n",
    "\n",
    "r2_data = pd.read_csv(r2_data_path)\n",
    "# print(r2_data.head())\n",
    "for (trait, p_val_threshold), group in r2_data.groupby(['trait', 'p_val_threshold']):\n",
    "    avg_incremental_r2 = group['r2_incremental'].mean()\n",
    "    res_df.append({\n",
    "        \"trait\": trait,\n",
    "        \"p_val_threshold\": p_val_threshold,\n",
    "        \"average_incremental_r2\": avg_incremental_r2\n",
    "    })\n",
    "res_df = pd.DataFrame(res_df)\n",
    "res_df = res_df.sort_values(by=['trait', 'average_incremental_r2'], ascending=[True, False])\n",
    "# display(res_df[res_df['trait'].str.contains('p30620')])\n",
    "# for each trait, get the best p_val_threshold\n",
    "best_res = res_df.loc[res_df.groupby('trait')['average_incremental_r2'].idxmax()]\n",
    "best_res = best_res.sort_values(by='trait')\n",
    "# display(best_res)\n",
    "print(best_res.shape)\n",
    "\n",
    "for row in best_res.itertuples(index=False):\n",
    "    trait = row.trait\n",
    "    trait_name = trait_dict[trait.split(\"_\")[0]]\n",
    "    # print(f\"Processing trait {trait_name} with p-value threshold {row.p_val_threshold}\")\n",
    "    gwas_path = os.path.join(full_eas_gwas_dir, f\"{trait_name}_int.{trait_name}_int.glm.linear\")\n",
    "    if trait_name in ['smoke', 'drink']:\n",
    "        gwas_path = os.path.join(full_eas_gwas_dir, f\"{trait_name}_raw.{trait_name}_raw.glm.logistic\")\n",
    "    p_val_threshold = row.p_val_threshold\n",
    "    output_prefix = os.path.join(output_base_dir, f\"EAS/{trait_name}\")\n",
    "    if not os.path.exists(os.path.dirname(output_prefix)):\n",
    "        os.makedirs(os.path.dirname(output_prefix))\n",
    "    command = f\"plink2 --bfile {eas_bfile_path} --clump {gwas_path} --clump-p1 {p_val_threshold} --clump-r2 0.1 --clump-kb 500 --out {output_prefix}\"\n",
    "    os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca29731",
   "metadata": {},
   "outputs": [],
   "source": [
    "plink2 --bfile /data1/jiapl_group/lishuhua/project/PRS_benchmark/software/tlprs/reference/EAS_1kg/1000G.EAS.QC.hm3.ind --clump /data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/CAS/gwas/gwas/alt_int.alt_int.glm.linear --clump-kb 500 --clump-p1 0.05 --clump-r2 0.1 --out /data1/jiapl_group/lishuhua/project/PRS_benchmark/software/ct/res/full_model/EAS/alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fcc02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EUR clump res demo: /data1/jiapl_group/lishuhua/project/PRS_benchmark/software/ct/res/clumped/alt/group_1/r2_0.1_w_500/ukb_chr10.clumps\n",
    "# EUR gwas demo: gwas_chr2.p30010_int.glm.linear\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "clump_res_dir = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/software/ct/res/clumped/\"\n",
    "eur_base_dir = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/Cross_Validation/UKB_EUR/train/gwas/\"\n",
    "output_dir = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/software/ct/res/filtered/\"\n",
    "\n",
    "p_val_list = [5e-8, 5e-7, 5e-6, 5e-5, 5e-4, 5e-3, 5e-2, 0.5, 1]\n",
    "\n",
    "trait_dict = {\n",
    "    'p48': 'waist',\n",
    "    'p50': 'height',\n",
    "    'p102': 'pulse',\n",
    "    'p4079': 'dbp',\n",
    "    'p4080': 'sbp',\n",
    "    'p20116': 'smoke',\n",
    "    'p20117': 'drink',\n",
    "    'p21001': 'bmi',\n",
    "    'p30000': 'wbc',\n",
    "    'p30010': 'rbc',\n",
    "    'p30020':'hb',\n",
    "    'p30080': 'plt',\n",
    "    'p30120': 'lymph',\n",
    "    'p30130': 'mono',\n",
    "    'p30140': 'neut',\n",
    "    'p30150': 'eos',\n",
    "    'p30620': 'alt',\n",
    "    'p30650': 'ast',\n",
    "    'p30670': 'bun',\n",
    "    'p30690': 'cholesterol',\n",
    "    'p30700': 'creatinine',\n",
    "    'p30730': 'ggt',\n",
    "    'p30740': 'glucose',\n",
    "    'p30760': 'hdl',\n",
    "    'p30780': 'ldl',\n",
    "    'p30870': 'triglycerides',\n",
    "    'p30880': 'ua'\n",
    "}\n",
    "\n",
    "for trait_code, trait in trait_dict.items():\n",
    "    for group in range(10, 11):\n",
    "        for chrom in range(1, 23):\n",
    "            print(f\"Processing trait {trait}, group {group}, chrom {chrom}\")\n",
    "            gwas_file = os.path.join(eur_base_dir, f\"fold_{group}/gwas_chr{chrom}.{trait_code}_int.glm.linear\")\n",
    "            if trait in ['smoke', 'drink']:\n",
    "                gwas_file = os.path.join(eur_base_dir, f\"fold_{group}/gwas_chr{chrom}_binary.{trait_code}_int.glm.linear\")\n",
    "            if not os.path.exists(gwas_file):\n",
    "                print(f\"File {gwas_file} does not exist. Skipping.\")\n",
    "                continue\n",
    "            gwas_df = pd.read_csv(gwas_file, sep=\"\\t\")\n",
    "            for p_val in p_val_list:\n",
    "                print()\n",
    "                clump_file = os.path.join(clump_res_dir, f\"{trait}/group_{group}/r2_0.1_w_500/ukb_chr{chrom}.clumps\")\n",
    "                if not os.path.exists(clump_file):\n",
    "                    print(f\"File {clump_file} does not exist. Skipping.\")\n",
    "                    continue\n",
    "                df = pd.read_csv(clump_file, sep=\"\\t\")\n",
    "                df_filtered = df[df['P'] <= p_val]\n",
    "                df_filtered = df_filtered[[\"#CHROM\", \"POS\", \"ID\"]]\n",
    "                res = pd.merge(df_filtered, gwas_df, on=[\"#CHROM\", \"POS\", \"ID\"], how='inner')\n",
    "                if res.shape[0] == 0:\n",
    "                    print(f\"No SNPs passed the p-value threshold {p_val} for trait {trait}, group {group}. Skipping.\")\n",
    "                    continue\n",
    "                output_subdir = os.path.join(output_dir, f\"{trait}/EUR/group_{group}/\")\n",
    "                os.makedirs(output_subdir, exist_ok=True)\n",
    "                output_file = os.path.join(output_subdir, f\"chr_{chrom}_pval_{p_val}.clumped\")\n",
    "                res.to_csv(output_file, sep=\"\\t\", index=False, header=True)\n",
    "                print(f\"Saved filtered clump file to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdc416c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "trait_dict = {\n",
    "    'p48': 'waist',\n",
    "    'p50': 'height',\n",
    "    'p102': 'pulse',\n",
    "    'p4079': 'dbp',\n",
    "    'p4080': 'sbp',\n",
    "    'p20116': 'smoke',\n",
    "    'p20117': 'drink',\n",
    "    'p21001': 'bmi',\n",
    "    'p30000': 'wbc',\n",
    "    'p30010': 'rbc',\n",
    "    'p30020':'hb',\n",
    "    'p30080': 'plt',\n",
    "    'p30120': 'lymph',\n",
    "    'p30130': 'mono',\n",
    "    'p30140': 'neut',\n",
    "    'p30150': 'eos',\n",
    "    'p30620': 'alt',\n",
    "    'p30650': 'ast',\n",
    "    'p30670': 'bun',\n",
    "    'p30690': 'cholesterol',\n",
    "    'p30700': 'creatinine',\n",
    "    'p30730': 'ggt',\n",
    "    'p30740': 'glucose',\n",
    "    'p30760': 'hdl',\n",
    "    'p30780': 'ldl',\n",
    "    'p30870': 'triglycerides',\n",
    "    'p30880': 'ua'\n",
    "}\n",
    "\n",
    "p_val_list = [5e-8, 5e-7, 5e-6, 5e-5, 5e-4, 5e-3, 5e-2, 0.5, 1]\n",
    "\n",
    "clump_res_dir = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/software/ct/res/filtered/\"\n",
    "eur_bfile_dir = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/Cross_Validation/UKB_EUR/test/\"\n",
    "output_dir = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/software/ct/res/test_in_sample/\"\n",
    "\n",
    "for trait_code, trait in trait_dict.items():\n",
    "    for group in range(1, 11):\n",
    "        for chrom in range(1, 23):\n",
    "            for p_val in p_val_list:\n",
    "                clump_file = os.path.join(clump_res_dir, f\"{trait}/EUR/group_{group}/chr_{chrom}_pval_{p_val}.clumped\")\n",
    "                if not os.path.exists(clump_file):\n",
    "                    print(f\"File {clump_file} does not exist. Skipping.\")\n",
    "                    continue\n",
    "                eur_bfile = os.path.join(eur_bfile_dir, f\"fold_{group}/chr{chrom}\")\n",
    "                if not os.path.exists(eur_bfile + \".bed\"):\n",
    "                    print(f\"File {eur_bfile}.bed does not exist. Skipping.\")\n",
    "                    continue\n",
    "                output_subdir = os.path.join(output_dir, f\"EUR/{trait}/group_{group}/\")\n",
    "                os.makedirs(output_subdir, exist_ok=True)\n",
    "                output_prefix = os.path.join(output_subdir, f\"chr_{chrom}_pval_{p_val}\")\n",
    "                prs_command = f\"plink2 --bfile {eur_bfile} --score {clump_file} 3 5 12 header no-mean-imputation --out {output_prefix}\"\n",
    "                os.system(prs_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2a3632",
   "metadata": {},
   "outputs": [],
   "source": [
    "### COMBINE ALL .sscore FILES INTO ONE .tsv FILE FOR EACH TRAIT\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "prs_base_dir = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/software/ct/res/test_in_sample/EUR/\"\n",
    "output_dir = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/software/ct/res/test_in_sample/EUR/\"\n",
    "trait_list = ['waist', 'height', 'pulse', 'dbp', 'sbp', 'smoke', 'drink', 'bmi', 'wbc', 'rbc', 'hb', 'plt', 'lymph', 'mono', 'neut', 'eos', 'alt', 'ast', 'bun', 'cholesterol', 'creatinine', 'glucose', 'ggt', 'hdl', 'ldl', 'triglycerides', 'ua']\n",
    "p_val_list = [5e-8, 5e-7, 5e-6, 5e-5, 5e-4, 5e-3, 5e-2, 0.5, 1]\n",
    "\n",
    "for trait in trait_list:\n",
    "    for group in range(1, 11):\n",
    "        for p_val in p_val_list:\n",
    "            print(f\"Processing trait {trait}, group {group}, p_val {p_val}\")\n",
    "            output_path = os.path.join(output_dir, f\"{trait}/group_{group}/\")\n",
    "            os.makedirs(output_path, exist_ok=True)\n",
    "            if os.path.exists(os.path.join(output_path, f\"pval_{p_val}.tsv\")):\n",
    "                print(f\"File {os.path.join(output_path, f'pval_{p_val}.tsv')} already exists. Skipping.\")\n",
    "                continue\n",
    "            all_chr_dfs = []\n",
    "            for chrom in range(1, 23):\n",
    "                prs_file_path = os.path.join(prs_base_dir, f\"{trait}/group_{group}/chr_{chrom}_pval_{p_val}.sscore\")\n",
    "                if not os.path.exists(prs_file_path):\n",
    "                    print(f\"File {prs_file_path} does not exist. Skipping.\")\n",
    "                    continue\n",
    "                prs_data = pd.read_csv(prs_file_path, sep='\\t')\n",
    "                all_chr_dfs.append(prs_data)\n",
    "            if not all_chr_dfs:\n",
    "                print(f\"No chromosome files found for trait {trait}, group {group}, p_val {p_val}. Skipping.\")\n",
    "                continue\n",
    "            combined_prs_df = pd.concat(all_chr_dfs, ignore_index=True)\n",
    "            combined_prs_df.to_csv(os.path.join(output_path, f\"pval_{p_val}.tsv\"), sep='\\t', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad581eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST FOR EUR IN SAMPLE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings that may arise from certain fits in statsmodels\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "warnings.simplefilter('ignore', ConvergenceWarning)\n",
    "\n",
    "# --- 1. Metric Calculation Functions ---\n",
    "\n",
    "def calculate_continuous_metrics(df, base_covars, full_covars):\n",
    "    \"\"\"Calculates all performance metrics for a continuous trait for a given dataset (df).\"\"\"\n",
    "    # Incremental RÂ²\n",
    "    model_base = sm.OLS(df[\"trait\"], sm.add_constant(df[base_covars])).fit()\n",
    "    model_full = sm.OLS(df[\"trait\"], sm.add_constant(df[full_covars])).fit()\n",
    "    r2_incremental = model_full.rsquared - model_base.rsquared\n",
    "\n",
    "    # Pearson correlation coefficient (SCORE vs. phenotype residuals)\n",
    "    pheno_residuals = model_base.resid\n",
    "    corr, _ = pearsonr(df[\"SCORE\"], pheno_residuals)\n",
    "\n",
    "    # RMSE\n",
    "    prediction_full = model_full.predict(sm.add_constant(df[full_covars]))\n",
    "    rmse = np.sqrt(mean_squared_error(df[\"trait\"], prediction_full))\n",
    "    nrmse_mean = rmse / df[\"trait\"].mean() if df[\"trait\"].mean() != 0 else np.nan\n",
    "    nrmse_range = rmse / (df[\"trait\"].max() - df[\"trait\"].min()) if (df[\"trait\"].max() - df[\"trait\"].min()) != 0 else np.nan\n",
    "    nrmse_std = rmse / df[\"trait\"].std() if df[\"trait\"].std() != 0 else np.nan\n",
    "\n",
    "    # Quantile means\n",
    "    df['quantile'] = pd.qcut(df['SCORE'], 5, labels=False, duplicates='drop')\n",
    "    quantile_means = df.groupby('quantile')['trait'].mean()\n",
    "    \n",
    "    return {\n",
    "        \"r2_incremental\": r2_incremental,\n",
    "        \"r2_full\": model_full.rsquared,\n",
    "        \"rmse\": rmse,\n",
    "        \"nrmse_mean\": nrmse_mean,\n",
    "        \"nrmse_range\": nrmse_range,\n",
    "        \"nrmse_std\": nrmse_std,\n",
    "        \"pearson_r\": corr,\n",
    "        \"top_quintile_mean\": quantile_means.iloc[-1] if not quantile_means.empty else np.nan,\n",
    "        \"bottom_quintile_mean\": quantile_means.iloc[0] if not quantile_means.empty else np.nan\n",
    "    }\n",
    "\n",
    "def calculate_binary_metrics(df, base_covars, full_covars):\n",
    "    \"\"\"Calculates all performance metrics for a binary trait for a given dataset (df).\"\"\"\n",
    "    # AUC and PR-AUC\n",
    "    logit_model = sm.Logit(df[\"trait\"], sm.add_constant(df[full_covars])).fit(disp=0)\n",
    "    pred_prob = logit_model.predict(sm.add_constant(df[full_covars]))\n",
    "    auc = roc_auc_score(df[\"trait\"], pred_prob)\n",
    "    pr_auc = average_precision_score(df[\"trait\"], pred_prob)\n",
    "\n",
    "    # OR per 1-SD\n",
    "    df[\"prs_scaled\"] = (df[\"SCORE\"] - df[\"SCORE\"].mean()) / df[\"SCORE\"].std()\n",
    "    logit_model_scaled = sm.Logit(df[\"trait\"], sm.add_constant(df[base_covars + [\"prs_scaled\"]])).fit(disp=0)\n",
    "    or_per_sd = np.exp(logit_model_scaled.params[\"prs_scaled\"])\n",
    "\n",
    "    # Quantile OR\n",
    "    df['prs_quintile'] = pd.qcut(df['SCORE'], 5, labels=False, duplicates='drop')\n",
    "    reference_quintile = 2 # Middle quintile\n",
    "    or_quintiles = {}\n",
    "    for q in range(5):\n",
    "        if q == reference_quintile:\n",
    "            or_quintiles[f'OR_Quintile_{q+1}'] = 1.0\n",
    "            continue\n",
    "        \n",
    "        # Check if both current and reference quintiles exist in the data\n",
    "        if not df['prs_quintile'].isin([q, reference_quintile]).all():\n",
    "             or_quintiles[f'OR_Quintile_{q+1}'] = np.nan\n",
    "             continue\n",
    "             \n",
    "        temp_df = df[df['prs_quintile'].isin([q, reference_quintile])].copy()\n",
    "        \n",
    "        # Check for sufficient data in both groups for stable model fitting\n",
    "        if temp_df['trait'].nunique() < 2 or temp_df['prs_quintile'].nunique() < 2:\n",
    "            or_quintiles[f'OR_Quintile_{q+1}'] = np.nan\n",
    "            continue\n",
    "            \n",
    "        temp_df['is_current_quintile'] = (temp_df['prs_quintile'] == q).astype(int)\n",
    "        X_quintile = sm.add_constant(temp_df[['is_current_quintile'] + base_covars])\n",
    "        try:\n",
    "            model_q = sm.Logit(temp_df[\"trait\"], X_quintile).fit(disp=0)\n",
    "            or_quintiles[f'OR_Quintile_{q+1}'] = np.exp(model_q.params['is_current_quintile'])\n",
    "        except Exception:\n",
    "            or_quintiles[f'OR_Quintile_{q+1}'] = np.nan\n",
    "            \n",
    "    results = {\n",
    "        \"auc\": auc,\n",
    "        \"pr_auc\": pr_auc,\n",
    "        \"or_per_sd\": or_per_sd,\n",
    "    }\n",
    "    results.update(or_quintiles) # Merge quantile ORs into the results dictionary\n",
    "    return results\n",
    "\n",
    "# --- 2. Main Execution Flow ---\n",
    "\n",
    "def main():\n",
    "    # --- Parameter Settings ---\n",
    "    # !! Note: Please modify the variables below according to your actual paths !!\n",
    "    cleaned_prs_path = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/software/ct/res/test_in_sample/\"\n",
    "    covar_path = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/UKB/pheno/covar/covars_white_british_final.tsv\"\n",
    "    output_dir = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/software/ct/res/test_in_sample/\"\n",
    "    pheno_dir = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/UKB/pheno/trait/White_British/\"\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # --- Initialization ---\n",
    "    final_results_continuous = []\n",
    "    covar_cols = [\"FID\", \"IID\", \"age\", \"sex\"] + [f\"PC{i}\" for i in range(1, 21)]\n",
    "    base_covars = [\"age\", \"sex\"] + [f\"PC{i}\" for i in range(1, 21)]\n",
    "    full_covars = base_covars + [\"SCORE\"]\n",
    "\n",
    "    # --- Data Loading and Processing ---\n",
    "    covars = pd.read_csv(covar_path, sep='\\t', usecols=covar_cols)\n",
    "\n",
    "    trait_dict = {\n",
    "        'p48': 'waist',\n",
    "        'p50': 'height',\n",
    "        'p102': 'pulse',\n",
    "        'p4079': 'dbp',\n",
    "        'p4080': 'sbp',\n",
    "        'p20116': 'smoke',\n",
    "        'p20117': 'drink',\n",
    "        'p21001': 'bmi',\n",
    "        'p30000': 'wbc',\n",
    "        'p30010': 'rbc',\n",
    "        'p30020':'hb',\n",
    "        'p30080': 'plt',\n",
    "        'p30120': 'lymph',\n",
    "        'p30130': 'mono',\n",
    "        'p30140': 'neut',\n",
    "        'p30150': 'eos',\n",
    "        'p30620': 'alt',\n",
    "        'p30650': 'ast',\n",
    "        'p30670': 'bun',\n",
    "        'p30690': 'cholesterol',\n",
    "        'p30700': 'creatinine',\n",
    "        'p30730': 'ggt',\n",
    "        'p30740': 'glucose',\n",
    "        'p30760': 'hdl',\n",
    "        'p30780': 'ldl',\n",
    "        'p30870': 'triglycerides',\n",
    "        'p30880': 'ua'\n",
    "    }\n",
    "    p_val_list = [5e-8, 5e-7, 5e-6, 5e-5, 5e-4, 5e-3, 5e-2, 0.5, 1]\n",
    "    # /data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/Cross_Validation/CAS/alt/group_1/pheno/test.txt\n",
    "\n",
    "    for trait, name in trait_dict.items():\n",
    "        for i in range(1, 11):\n",
    "            for p_val in p_val_list:\n",
    "                trait_prs_path = os.path.join(cleaned_prs_path, f\"EUR/{name}/group_{i}/pval_{p_val}.tsv\")\n",
    "                if not os.path.exists(trait_prs_path):\n",
    "                    print(f\"Warning: PRS file for trait {name} group {i} not found. Skipping.\")\n",
    "                    continue\n",
    "                print(f\"\\nProcessing Trait: {name}, Group: {i}, P-value: {p_val}\")\n",
    "                if trait == 'p20116' or trait == 'p20117':\n",
    "                    trait_id_from_prs = f\"{trait}_int\"\n",
    "                else:\n",
    "                    trait_id_from_prs = f\"{trait}_raw\"\n",
    "                pheno_path = os.path.join(pheno_dir, f\"{trait_id_from_prs}.txt\")\n",
    "                pheno = pd.read_csv(pheno_path, sep='\\t')\n",
    "                pheno.columns = [\"FID\", \"IID\", \"trait\"]\n",
    "                print(pheno.head())\n",
    "                prs = pd.read_csv(trait_prs_path, sep='\\t')\n",
    "                prs.rename(columns={\"#FID\": \"FID\"}, inplace=True)\n",
    "                prs.rename(columns={\"SCORE1_AVG\": \"SCORE\"}, inplace=True)\n",
    "\n",
    "                for df in [pheno, prs, covars]:\n",
    "                    df[\"FID\"] = df[\"FID\"].astype(str)\n",
    "                    df[\"IID\"] = df[\"IID\"].astype(str)\n",
    "                merged_data = pd.merge(pheno, covars, on=[\"FID\", \"IID\"], how=\"inner\")\n",
    "                merged_data = pd.merge(merged_data, prs, on=[\"FID\", \"IID\"], how=\"inner\")\n",
    "                print(merged_data.head())\n",
    "                # Defensive data cleaning and type conversion\n",
    "                numeric_cols = [\"trait\", \"SCORE\", \"age\", \"sex\"] + [f\"PC{i}\" for i in range(1, 11)]\n",
    "                for col in numeric_cols:\n",
    "                    if col in merged_data.columns:\n",
    "                        merged_data[col] = pd.to_numeric(merged_data[col], errors='coerce')\n",
    "\n",
    "                original_rows = len(merged_data)\n",
    "                merged_data.dropna(subset=numeric_cols, inplace=True)\n",
    "                new_rows = len(merged_data)\n",
    "                if original_rows > new_rows:\n",
    "                    print(f\"--> Warning: Dropped {original_rows - new_rows} rows due to non-numeric data or NaNs in trait {trait_id_from_prs}.\")\n",
    "                if new_rows == 0:\n",
    "                    print(f\"--> Error: No valid samples remained for trait {trait_id_from_prs} after cleaning. Skipping.\")\n",
    "                    continue\n",
    "                print(f\"Data merged and cleaned for trait {trait_id_from_prs}. Total samples: {len(merged_data)}\")\n",
    "\n",
    "                if trait_id_from_prs in [\"p20116_int\", \"p20117_int\"]:\n",
    "                    # Binary trait analysis\n",
    "                    # Ensure binary trait is 0/1 coded\n",
    "                    unique_vals = sorted(merged_data[\"trait\"].unique())\n",
    "                    if not set(unique_vals).issubset({0, 1}):\n",
    "                        if len(unique_vals) == 2:\n",
    "                            print(f\"Converting binary trait from {unique_vals} to 0/1.\")\n",
    "                            merged_data[\"trait\"] = (merged_data[\"trait\"] == unique_vals[1]).astype(int)\n",
    "                        else:\n",
    "                            print(f\"Error: Binary trait column for {trait_id_from_prs} contains unexpected values: {unique_vals}. Skipping.\")\n",
    "                            continue\n",
    "                        \n",
    "                    # Direct calculation of metrics\n",
    "                    analysis_report = calculate_continuous_metrics(merged_data, base_covars, full_covars)\n",
    "                    analysis_report['trait'] = trait_id_from_prs\n",
    "                    analysis_report['p_val_threshold'] = p_val\n",
    "                    final_results_continuous.append(analysis_report)\n",
    "                else:\n",
    "                    # Continuous trait analysis\n",
    "                    # Direct calculation of metrics\n",
    "                    analysis_report = calculate_continuous_metrics(merged_data, base_covars, full_covars)\n",
    "                    analysis_report['trait'] = trait_id_from_prs\n",
    "                    analysis_report['p_val_threshold'] = p_val\n",
    "                    final_results_continuous.append(analysis_report)\n",
    "\n",
    "        # --- 3. Save Final Results ---\n",
    "        if final_results_continuous:\n",
    "            continuous_df = pd.DataFrame(final_results_continuous)\n",
    "            # Reorder columns to have 'trait' first\n",
    "            cols = ['trait'] + [col for col in continuous_df.columns if col != 'trait']\n",
    "            continuous_df = continuous_df[cols]\n",
    "            continuous_df.to_csv(os.path.join(output_dir, \"EUR_in_sample_metrics.csv\"), index=False)\n",
    "            print(\"\\nContinuous trait results saved to EUR_in_sample_metrics.csv\")\n",
    "            print(continuous_df)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94f2026a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trait</th>\n",
       "      <th>p_val_threshold</th>\n",
       "      <th>average_incremental_r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>p102_raw</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>p4079_raw</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.000901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>p4080_raw</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>p48_raw</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.001428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>p50_raw</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.003941</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        trait  p_val_threshold  average_incremental_r2\n",
       "6    p102_raw             0.05                0.000983\n",
       "16  p4079_raw             0.50                0.000901\n",
       "24  p4080_raw             0.05                0.000997\n",
       "35    p48_raw             1.00                0.001428\n",
       "42    p50_raw             0.05                0.003941"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# r2_data_path = \"../../../../PRS_benchmark/data/result/real_data/ct/EUR_in_sample_metrics.csv\"\n",
    "r2_data_path = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/software/ct/res/test_in_sample/EUR_in_sample_metrics.csv\"\n",
    "eur_bfile_dir = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/UKB/geno/White_British/0_sample_qc/\"\n",
    "full_eur_gwas_path = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/UKB/gwas/White_British/\"\n",
    "res_df = []\n",
    "output_base_dir = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/software/ct/res/full_model/\"\n",
    "\n",
    "trait_dict = {\n",
    "        'p48': 'waist',\n",
    "        'p50': 'height',\n",
    "        'p102': 'pulse',\n",
    "        'p4079': 'dbp',\n",
    "        'p4080': 'sbp',\n",
    "        'p20116': 'smoke',\n",
    "        'p20117': 'drink',\n",
    "        'p21001': 'bmi',\n",
    "        'p30000': 'wbc',\n",
    "        'p30010': 'rbc',\n",
    "        'p30020':'hb',\n",
    "        'p30080': 'plt',\n",
    "        'p30120': 'lymph',\n",
    "        'p30130': 'mono',\n",
    "        'p30140': 'neut',\n",
    "        'p30150': 'eos',\n",
    "        'p30620': 'alt',\n",
    "        'p30650': 'ast',\n",
    "        'p30670': 'bun',\n",
    "        'p30690': 'cholesterol',\n",
    "        'p30700': 'creatinine',\n",
    "        'p30730': 'ggt',\n",
    "        'p30740': 'glucose',\n",
    "        'p30760': 'hdl',\n",
    "        'p30780': 'ldl',\n",
    "        'p30870': 'triglycerides',\n",
    "        'p30880': 'ua'\n",
    "    }\n",
    "\n",
    "r2_data = pd.read_csv(r2_data_path)\n",
    "for (trait, p_val_threshold), group in r2_data.groupby(['trait', 'p_val_threshold']):\n",
    "    avg_incremental_r2 = group['r2_incremental'].mean()\n",
    "    res_df.append({\n",
    "        \"trait\": trait,\n",
    "        \"p_val_threshold\": p_val_threshold,\n",
    "        \"average_incremental_r2\": avg_incremental_r2\n",
    "    })\n",
    "res_df = pd.DataFrame(res_df)\n",
    "res_df = res_df.sort_values(by=['trait', 'average_incremental_r2'], ascending=[True, False])\n",
    "# for each trait, get the best p_val_threshold\n",
    "best_res = res_df.loc[res_df.groupby('trait')['average_incremental_r2'].idxmax()]\n",
    "best_res = best_res.sort_values(by='trait')\n",
    "# display(best_res)\n",
    "print(best_res.shape)\n",
    "\n",
    "for row in best_res.itertuples(index=False):\n",
    "    # p102_int_chr1.p102_int.glm.linear\n",
    "    # p20116_int_chr1.p20116_int.glm.logistic\n",
    "    trait = row.trait\n",
    "    trait = trait.replace(\"_raw\", \"\").replace(\"_int\", \"\")\n",
    "    trait_name = trait_dict[trait]\n",
    "    p_val_threshold = row.p_val_threshold\n",
    "    for chrom in range(1, 23):\n",
    "        print(f\"Processing trait {trait_name} with p-value threshold {p_val_threshold} on chr {chrom}\")\n",
    "        eur_bfile_path = os.path.join(eur_bfile_dir, f\"chr{chrom}\")\n",
    "        gwas_path = os.path.join(full_eur_gwas_path, f\"{trait}_int_chr{chrom}.{trait}_int.glm.linear\")\n",
    "        if trait_name in ['smoke', 'drink']:\n",
    "            gwas_path = os.path.join(full_eur_gwas_path, f\"{trait}_int_chr{chrom}.{trait}_int.glm.logistic\")\n",
    "        output_prefix = os.path.join(output_base_dir, f\"EUR/{trait_name}/chr_{chrom}\")\n",
    "        if not os.path.exists(os.path.dirname(output_prefix)):\n",
    "            os.makedirs(os.path.dirname(output_prefix))\n",
    "        command = f\"plink2 --bfile {eur_bfile_path} --clump {gwas_path} --rm-dup exclude-mismatch --clump-p1 {p_val_threshold} --clump-r2 0.1 --clump-kb 500 --out {output_prefix}\"\n",
    "        os.system(command)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
