{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db299ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: get EAS summary statistics\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# eas_example: /data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/Cross_Validation/CAS/bmi/group_1/gwas/train.Pheno.glm.linear\n",
    "\n",
    "# sig_snps = df[[\"ID\", \"#CHROM\", \"ALT\", \"REF\", \"BETA\", \"SE\", \"OBS_CT\"]]\n",
    "# sig_snps.columns = [\"rsid\", \"chr\", \"a1\", \"a0\", \"beta\", \"beta_se\", \"n_eff\"]\n",
    "\n",
    "eas_base_dir = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/Cross_Validation/CAS/\"\n",
    "eur_base_dir = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/Cross_Validation/UKB_EUR/\"\n",
    "output_gwas_dir = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/software/prosper/train/\"\n",
    "snp_list_path = \"/data1/jiapl_group/lishuhua/software/PRS/PROSPER/reference/ref_bim.txt\"\n",
    "\n",
    "trait_list = ['waist', 'height', 'pulse', 'dbp', 'sbp', 'smoke', 'drink', 'bmi', 'wbc', 'rbc', 'hb', 'plt', 'lymph', 'mono', 'neut', 'eos', 'alt', 'ast', 'bun', 'cholesterol', 'creatinine', 'glucose', 'ggt', 'hdl', 'ldl', 'triglycerides', 'ua']\n",
    "\n",
    "def get_summary_stats(trait, group_num, gwas_path, output_path, snp_list):\n",
    "    print(f\"Processing trait: {trait}, group: {group_num}\")\n",
    "    if \"BETA\" in pd.read_csv(gwas_path, sep=\"\\t\", nrows=1).columns:\n",
    "        # df = df[[\"ID\", \"#CHROM\", \"ALT\", \"REF\", \"BETA\", \"SE\", \"OBS_CT\"]]\n",
    "        df = pd.read_csv(gwas_path, sep='\\t', usecols=['ID', '#CHROM', 'ALT', 'REF', 'BETA', 'SE', 'OBS_CT'])\n",
    "    else:\n",
    "        df = pd.read_csv(gwas_path, sep='\\t', usecols=['ID', '#CHROM', 'ALT', 'REF', 'OR', 'LOG(OR)_SE', 'OBS_CT'])\n",
    "        df['BETA'] = np.log(df['OR'])\n",
    "        df['SE'] = df['LOG(OR)_SE']\n",
    "    df = df[[\"ID\", \"#CHROM\", \"ALT\", \"REF\", \"BETA\", \"SE\", \"OBS_CT\"]]\n",
    "    df.columns = [\"rsid\", \"chr\", \"a1\", \"a0\", \"beta\", \"beta_se\", \"n_eff\"]\n",
    "    merge_df = pd.merge(df, snp_list, on=[\"chr\", \"rsid\"], how=\"inner\")\n",
    "    if merge_df.empty:\n",
    "        print(f\"No SNPs found for trait: {trait}, group: {group_num}\")\n",
    "        return\n",
    "    else:\n",
    "        print(f\"Found {len(merge_df)} SNPs for trait: {trait}, group: {group_num}\")\n",
    "    merge_df = merge_df[['rsid', 'chr', 'a1', 'a0', 'beta', 'beta_se', 'n_eff']]\n",
    "    merge_df = merge_df.drop_duplicates(subset=['rsid'])\n",
    "    merge_df = merge_df.dropna(subset=['rsid', 'beta', 'beta_se', 'n_eff'])\n",
    "    merge_df.to_csv(output_path, sep=\"\\t\", index=False, header=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    snp_list = pd.read_csv(snp_list_path, sep=\"\\t\", header=None, usecols=[0,1])\n",
    "    snp_list.columns = [\"chr\", \"rsid\"]\n",
    "    for trait in trait_list:\n",
    "        for group_num in range(1, 11):\n",
    "            eas_gwas_path = os.path.join(eas_base_dir, trait, f\"group_{group_num}\", \"gwas\", \"train.Pheno.glm.linear\")\n",
    "            if os.path.exists(eas_gwas_path):\n",
    "                eas_output_path = os.path.join(output_gwas_dir, \"EAS\", trait, f\"group_{group_num}.txt\")\n",
    "                if not os.path.exists(os.path.dirname(eas_output_path)):\n",
    "                    os.makedirs(os.path.dirname(eas_output_path))\n",
    "                get_summary_stats(trait, group_num, eas_gwas_path, eas_output_path, snp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd29c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: get EAS summary statistics\n",
    "# !!! Only change for smoke and drink !!!\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# eas_example: /data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/Cross_Validation/CAS/bmi/group_1/gwas/train.Pheno.glm.linear\n",
    "# eur_example: /data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/Cross_Validation/UKB_EUR/height/group_3/merged_gwas/height.merged.glm.linear\n",
    "\n",
    "# sig_snps = df[[\"ID\", \"#CHROM\", \"ALT\", \"REF\", \"BETA\", \"SE\", \"OBS_CT\"]]\n",
    "# sig_snps.columns = [\"rsid\", \"chr\", \"a1\", \"a0\", \"beta\", \"beta_se\", \"n_eff\"]\n",
    "\n",
    "eas_base_dir = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/Cross_Validation/CAS/\"\n",
    "eur_base_dir = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/Cross_Validation/UKB_EUR/\"\n",
    "output_gwas_dir = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/software/prosper/train/\"\n",
    "snp_list_path = \"/data1/jiapl_group/lishuhua/software/PRS/PROSPER/reference/ref_bim.txt\"\n",
    "\n",
    "# trait_list = ['waist', 'height', 'pulse', 'dbp', 'sbp', 'smoke', 'drink', 'bmi', 'wbc', 'rbc', 'hb', 'plt', 'lymph', 'mono', 'neut', 'eos', 'alt', 'ast', 'bun', 'cholesterol', 'creatinine', 'glucose', 'ggt', 'hdl', 'ldl', 'triglycerides', 'ua']\n",
    "trait_list = ['drink', 'smoke']\n",
    "\n",
    "def get_summary_stats(trait, group_num, gwas_path, output_path, snp_list, n_control=None, n_case=None):\n",
    "    print(f\"Processing trait: {trait}, group: {group_num}\")\n",
    "    if \"BETA\" in pd.read_csv(gwas_path, sep=\"\\t\", nrows=1).columns:\n",
    "        # df = df[[\"ID\", \"#CHROM\", \"ALT\", \"REF\", \"BETA\", \"SE\", \"OBS_CT\"]]\n",
    "        df = pd.read_csv(gwas_path, sep='\\t', usecols=['ID', '#CHROM', 'ALT', 'REF', 'BETA', 'SE', 'OBS_CT'])\n",
    "    else:\n",
    "        df = pd.read_csv(gwas_path, sep='\\t', usecols=['ID', '#CHROM', 'ALT', 'REF', 'OR', 'LOG(OR)_SE', 'OBS_CT'])\n",
    "        df['BETA'] = np.log(df['OR'])\n",
    "        df['SE'] = df['LOG(OR)_SE']\n",
    "    df = df[[\"ID\", \"#CHROM\", \"ALT\", \"REF\", \"BETA\", \"SE\", \"OBS_CT\"]]\n",
    "    df.columns = [\"rsid\", \"chr\", \"a1\", \"a0\", \"beta\", \"beta_se\", \"n_eff\"]\n",
    "    if n_control is not None and n_case is not None:\n",
    "        df['n_eff'] = 4 / (1 / n_control + 1 / n_case)\n",
    "    merge_df = pd.merge(df, snp_list, on=[\"chr\", \"rsid\"], how=\"inner\")\n",
    "    if merge_df.empty:\n",
    "        print(f\"No SNPs found for trait: {trait}, group: {group_num}\")\n",
    "        return\n",
    "    else:\n",
    "        print(f\"Found {len(merge_df)} SNPs for trait: {trait}, group: {group_num}\")\n",
    "    merge_df = merge_df[['rsid', 'chr', 'a1', 'a0', 'beta', 'beta_se', 'n_eff']]\n",
    "    merge_df = merge_df.drop_duplicates(subset=['rsid'])\n",
    "    merge_df = merge_df.dropna(subset=['rsid', 'beta', 'beta_se', 'n_eff'])\n",
    "    merge_df.to_csv(output_path, sep=\"\\t\", index=False, header=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    snp_list = pd.read_csv(snp_list_path, sep=\"\\t\", header=None, usecols=[0,1])\n",
    "    snp_list.columns = [\"chr\", \"rsid\"]\n",
    "    for trait in trait_list:\n",
    "        for group_num in range(1, 11):\n",
    "            eas_gwas_path = os.path.join(eas_base_dir, trait, f\"group_{group_num}\", \"gwas\", \"train.Pheno.glm.linear\")\n",
    "            if trait == \"smoke\" or trait == \"drink\":\n",
    "                eas_gwas_path = os.path.join(eas_base_dir, trait, f\"group_{group_num}\", \"gwas\", \"train.Pheno.glm.logistic\")\n",
    "            \n",
    "            if os.path.exists(eas_gwas_path):\n",
    "                eas_output_path = os.path.join(output_gwas_dir, \"EAS\", trait, f\"group_{group_num}.txt\")\n",
    "                eas_train_pheno_path = os.path.join(eas_base_dir, trait, f\"group_{group_num}\", \"pheno\", \"train_pheno.txt\")\n",
    "                if os.path.exists(eas_train_pheno_path):\n",
    "                    eas_train_pheno = pd.read_csv(eas_train_pheno_path, sep=\"\\t\", header=0)\n",
    "                    eas_train_pheno['Pheno'] = eas_train_pheno['Pheno'].astype(int)\n",
    "                    n_control = eas_train_pheno[eas_train_pheno['Pheno'] == 1].shape[0]\n",
    "                    n_case = eas_train_pheno[eas_train_pheno['Pheno'] == 2].shape[0]\n",
    "                if not os.path.exists(os.path.dirname(eas_output_path)):\n",
    "                    os.makedirs(os.path.dirname(eas_output_path))\n",
    "                get_summary_stats(trait, group_num, eas_gwas_path, eas_output_path, snp_list, n_control, n_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0f4132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: get EAS summary statistics\n",
    "# !!! merge the code for continuous and binary traits !!!\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# 定义基础目录和文件路径\n",
    "eas_base_dir = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/Cross_Validation/CAS/\"\n",
    "output_gwas_dir = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/software/prosper/train/\"\n",
    "snp_list_path = \"/data1/jiapl_group/lishuhua/software/PRS/PROSPER/reference/ref_bim.txt\"\n",
    "\n",
    "# 包含所有连续性和二元性状的列表\n",
    "trait_list = [\n",
    "    'waist', 'height', 'pulse', 'dbp', 'sbp', 'smoke', 'drink', 'bmi', 'wbc', \n",
    "    'rbc', 'hb', 'plt', 'lymph', 'mono', 'neut', 'eos', 'alt', 'ast', 'bun', \n",
    "    'cholesterol', 'creatinine', 'glucose', 'ggt', 'hdl', 'ldl', 'triglycerides', 'ua'\n",
    "]\n",
    "\n",
    "# 定义二元性状列表，用于特殊处理\n",
    "binary_traits = ['smoke', 'drink']\n",
    "\n",
    "def get_summary_stats(trait, group_num, gwas_path, output_path, snp_list, n_control=None, n_case=None):\n",
    "    \"\"\"\n",
    "    处理GWAS摘要统计数据，适用于连续性和二元性状。\n",
    "    \n",
    "    对于二元性状，如果提供了 n_control 和 n_case，则会重新计算有效样本量 (n_eff)。\n",
    "    \"\"\"\n",
    "    print(f\"Processing trait: {trait}, group: {group_num}\")\n",
    "    \n",
    "    # 检查摘要统计文件中是包含 'BETA' 还是 'OR'\n",
    "    # 'BETA' 通常用于连续性状 (线性回归)，'OR' 用于二元性状 (逻辑回归)\n",
    "    header = pd.read_csv(gwas_path, sep=\"\\t\", nrows=1).columns\n",
    "    if \"BETA\" in header:\n",
    "        use_cols = ['ID', '#CHROM', 'ALT', 'REF', 'BETA', 'SE', 'OBS_CT']\n",
    "        df = pd.read_csv(gwas_path, sep='\\t', usecols=use_cols)\n",
    "    else:\n",
    "        use_cols = ['ID', '#CHROM', 'ALT', 'REF', 'OR', 'LOG(OR)_SE', 'OBS_CT']\n",
    "        df = pd.read_csv(gwas_path, sep='\\t', usecols=use_cols)\n",
    "        # 将 OR (优势比) 转换为 BETA (log(OR))\n",
    "        df['BETA'] = np.log(df['OR'])\n",
    "        df['SE'] = df['LOG(OR)_SE']\n",
    "        \n",
    "    # 标准化列名\n",
    "    df = df[[\"ID\", \"#CHROM\", \"ALT\", \"REF\", \"BETA\", \"SE\", \"OBS_CT\"]]\n",
    "    df.columns = [\"rsid\", \"chr\", \"a1\", \"a0\", \"beta\", \"beta_se\", \"n_eff\"]\n",
    "    \n",
    "    # 如果是二元性状，根据病例和对照数计算有效样本量\n",
    "    if n_control is not None and n_case is not None:\n",
    "        if n_control > 0 and n_case > 0:\n",
    "            df['n_eff'] = 4 / (1 / n_control + 1 / n_case)\n",
    "        else:\n",
    "            print(f\"Warning: Zero cases or controls for {trait}, group {group_num}. n_eff not recalculated.\")\n",
    "\n",
    "    # 与参考SNP列表合并，筛选有效SNP\n",
    "    merge_df = pd.merge(df, snp_list, on=[\"chr\", \"rsid\"], how=\"inner\")\n",
    "    \n",
    "    if merge_df.empty:\n",
    "        print(f\"No SNPs found after merging for trait: {trait}, group: {group_num}\")\n",
    "        return\n",
    "    else:\n",
    "        print(f\"Found {len(merge_df)} SNPs for trait: {trait}, group: {group_num}\")\n",
    "    \n",
    "    # 数据清洗和整理\n",
    "    merge_df = merge_df[['rsid', 'chr', 'a1', 'a0', 'beta', 'beta_se', 'n_eff']]\n",
    "    merge_df = merge_df.drop_duplicates(subset=['rsid'])\n",
    "    merge_df = merge_df.dropna(subset=['rsid', 'beta', 'beta_se', 'n_eff'])\n",
    "    \n",
    "    # 保存处理后的结果\n",
    "    merge_df.to_csv(output_path, sep=\"\\t\", index=False, header=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 加载参考SNP列表\n",
    "    snp_list = pd.read_csv(snp_list_path, sep=\"\\t\", header=None, usecols=[0, 1])\n",
    "    snp_list.columns = [\"chr\", \"rsid\"]\n",
    "\n",
    "    # 遍历所有性状和分组\n",
    "    for trait in trait_list:\n",
    "        for group_num in range(1, 11):\n",
    "            n_control, n_case = None, None # 默认为None，适用于连续性状\n",
    "            \n",
    "            # 判断性状类型并设置相应的文件名和参数\n",
    "            if trait in binary_traits:\n",
    "                # 处理二元性状\n",
    "                file_suffix = \"logistic\"\n",
    "                eas_train_pheno_path = os.path.join(eas_base_dir, trait, f\"group_{group_num}\", \"pheno\", \"train_pheno.txt\")\n",
    "                \n",
    "                if os.path.exists(eas_train_pheno_path):\n",
    "                    eas_train_pheno = pd.read_csv(eas_train_pheno_path, sep=\"\\t\", header=0)\n",
    "                    eas_train_pheno['Pheno'] = eas_train_pheno['Pheno'].astype(int)\n",
    "                    # 计算病例和对照的数量 (假设 1=对照, 2=病例)\n",
    "                    n_control = eas_train_pheno[eas_train_pheno['Pheno'] == 1].shape[0]\n",
    "                    n_case = eas_train_pheno[eas_train_pheno['Pheno'] == 2].shape[0]\n",
    "                else:\n",
    "                    print(f\"Phenotype file not found for {trait}, group {group_num}. Cannot calculate n_eff.\")\n",
    "                    continue\n",
    "            else:\n",
    "                # 处理连续性状\n",
    "                file_suffix = \"linear\"\n",
    "\n",
    "            # 构建GWAS摘要统计文件路径\n",
    "            gwas_filename = f\"train.Pheno.glm.{file_suffix}\"\n",
    "            eas_gwas_path = os.path.join(eas_base_dir, trait, f\"group_{group_num}\", \"gwas\", gwas_filename)\n",
    "\n",
    "            if os.path.exists(eas_gwas_path):\n",
    "                # 构建输出文件路径并确保目录存在\n",
    "                eas_output_path = os.path.join(output_gwas_dir, \"EAS\", trait, f\"group_{group_num}.txt\")\n",
    "                os.makedirs(os.path.dirname(eas_output_path), exist_ok=True)\n",
    "                \n",
    "                # 调用统一的处理函数\n",
    "                get_summary_stats(trait, group_num, eas_gwas_path, eas_output_path, snp_list, n_control, n_case)\n",
    "            else:\n",
    "                print(f\"GWAS file not found: {eas_gwas_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c398ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: get EUR summary statistics\n",
    "# !!! merge the code for continuous and binary traits !!!\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# 定义基础目录和文件路径\n",
    "eur_base_dir = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/Cross_Validation/UKB_EUR/\"\n",
    "output_gwas_dir = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/software/prosper/train/\"\n",
    "snp_list_path = \"/data1/jiapl_group/lishuhua/software/PRS/PROSPER/reference/ref_bim.txt\"\n",
    "\n",
    "# 包含所有连续性和二元性状的列表\n",
    "trait_dict = {\n",
    "        # 'p48': 'waist',\n",
    "        # 'p50': 'height',\n",
    "        # 'p102': 'pulse',\n",
    "        # 'p4079': 'dbp',\n",
    "        # 'p4080': 'sbp',\n",
    "        'p20116': 'smoke',\n",
    "        'p20117': 'drink',\n",
    "        # 'p21001': 'bmi',\n",
    "        # 'p30000': 'wbc',\n",
    "        # 'p30010': 'rbc',\n",
    "        # 'p30020':'hb',\n",
    "        # 'p30080': 'plt',\n",
    "        # 'p30120': 'lymph',\n",
    "        # 'p30130': 'mono',\n",
    "        # 'p30140': 'neut',\n",
    "        # 'p30150': 'eos',\n",
    "        # 'p30620': 'alt',\n",
    "        # 'p30650': 'ast',\n",
    "        # 'p30670': 'bun',\n",
    "        # 'p30690': 'cholesterol',\n",
    "        # 'p30700': 'creatinine',\n",
    "        # 'p30730': 'ggt',\n",
    "        # 'p30740': 'glucose',\n",
    "        # 'p30760': 'hdl',\n",
    "        # 'p30780': 'ldl',\n",
    "        # 'p30870': 'triglycerides',\n",
    "        # 'p30880': 'ua'\n",
    "    }\n",
    "# 定义二元性状列表，用于特殊处理\n",
    "binary_traits = ['smoke', 'drink']\n",
    "\n",
    "def get_summary_stats(trait, group_num, gwas_path, output_path, snp_list, n_control=None, n_case=None):\n",
    "    \"\"\"\n",
    "    处理GWAS摘要统计数据，适用于连续性和二元性状。\n",
    "    \n",
    "    对于二元性状，如果提供了 n_control 和 n_case，则会重新计算有效样本量 (n_eff)。\n",
    "    \"\"\"\n",
    "    print(f\"Processing trait: {trait}, group: {group_num}\")\n",
    "    \n",
    "    # 检查摘要统计文件中是包含 'BETA' 还是 'OR'\n",
    "    # 'BETA' 通常用于连续性状 (线性回归)，'OR' 用于二元性状 (逻辑回归)\n",
    "    header = pd.read_csv(gwas_path, sep=\"\\t\", nrows=1).columns\n",
    "    if \"BETA\" in header:\n",
    "        use_cols = ['ID', '#CHROM', 'ALT', 'REF', 'BETA', 'SE', 'OBS_CT']\n",
    "        df = pd.read_csv(gwas_path, sep='\\t', usecols=use_cols)\n",
    "    else:\n",
    "        use_cols = ['ID', '#CHROM', 'ALT', 'REF', 'OR', 'LOG(OR)_SE', 'OBS_CT']\n",
    "        df = pd.read_csv(gwas_path, sep='\\t', usecols=use_cols)\n",
    "        # 将 OR (优势比) 转换为 BETA (log(OR))\n",
    "        df['BETA'] = np.log(df['OR'])\n",
    "        df['SE'] = df['LOG(OR)_SE']\n",
    "        \n",
    "    # 标准化列名\n",
    "    df = df[[\"ID\", \"#CHROM\", \"ALT\", \"REF\", \"BETA\", \"SE\", \"OBS_CT\"]]\n",
    "    df.columns = [\"rsid\", \"chr\", \"a1\", \"a0\", \"beta\", \"beta_se\", \"n_eff\"]\n",
    "    \n",
    "    # 如果是二元性状，根据病例和对照数计算有效样本量\n",
    "    if n_control is not None and n_case is not None:\n",
    "        if n_control > 0 and n_case > 0:\n",
    "            df['n_eff'] = 4 / (1 / n_control + 1 / n_case)\n",
    "        else:\n",
    "            print(f\"Warning: Zero cases or controls for {trait}, group {group_num}. n_eff not recalculated.\")\n",
    "\n",
    "    # 与参考SNP列表合并，筛选有效SNP\n",
    "    merge_df = pd.merge(df, snp_list, on=[\"chr\", \"rsid\"], how=\"inner\")\n",
    "    \n",
    "    if merge_df.empty:\n",
    "        print(f\"No SNPs found after merging for trait: {trait}, group: {group_num}\")\n",
    "        return\n",
    "    else:\n",
    "        print(f\"Found {len(merge_df)} SNPs for trait: {trait}, group: {group_num}\")\n",
    "    \n",
    "    # 数据清洗和整理\n",
    "    merge_df = merge_df[['rsid', 'chr', 'a1', 'a0', 'beta', 'beta_se', 'n_eff']]\n",
    "    merge_df = merge_df.drop_duplicates(subset=['rsid'])\n",
    "    merge_df = merge_df.dropna(subset=['rsid', 'beta', 'beta_se', 'n_eff'])\n",
    "    \n",
    "    # 保存处理后的结果\n",
    "    merge_df.to_csv(output_path, sep=\"\\t\", index=False, header=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 加载参考SNP列表\n",
    "    snp_list = pd.read_csv(snp_list_path, sep=\"\\t\", header=None, usecols=[0, 1])\n",
    "    snp_list.columns = [\"chr\", \"rsid\"]\n",
    "\n",
    "    # 遍历所有性状和分组\n",
    "    for trait_id, trait in trait_dict.items():\n",
    "        trait_id = trait_id + \"_int\"\n",
    "        for group_num in range(1, 11):\n",
    "            n_control, n_case = None, None # 默认为None，适用于连续性状\n",
    "            # 判断性状类型并设置相应的文件名和参数\n",
    "            # /data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/Cross_Validation/UKB_EUR/pheno/fold_1/train_pheno_int_dropna_2.txt\n",
    "            if trait in binary_traits:\n",
    "                # 处理二元性状\n",
    "                file_suffix = \"logistic\"\n",
    "                chrom_prefix = \"_binary\"\n",
    "                eur_train_pheno_path = os.path.join(eur_base_dir, f\"pheno/fold_{group_num}\", \"train_pheno_int_dropna_2.txt\")\n",
    "                \n",
    "                if os.path.exists(eur_train_pheno_path):\n",
    "                    eur_train_pheno = pd.read_csv(eur_train_pheno_path, sep=\"\\t\", header=0)\n",
    "                    eur_train_pheno[trait_id] = eur_train_pheno[trait_id].astype(int)\n",
    "                    # 计算病例和对照的数量 (假设 1=对照, 2=病例)\n",
    "                    n_control = eur_train_pheno[eur_train_pheno[trait_id] == 1].shape[0]\n",
    "                    n_case = eur_train_pheno[eur_train_pheno[trait_id] == 2].shape[0]\n",
    "                else:\n",
    "                    print(f\"Phenotype file not found for {trait}, group {group_num}. Cannot calculate n_eff.\")\n",
    "                    continue\n",
    "            else:\n",
    "                # 处理连续性状\n",
    "                file_suffix = \"linear\"\n",
    "                chrom_prefix = \"\"\n",
    "\n",
    "            # 构建GWAS摘要统计文件路径\n",
    "            for chrom in range(1, 23):\n",
    "                # eur_example: /data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/Cross_Validation/UKB_EUR/train/gwas/fold_1/gwas_chr1.p102_int.glm.linear\n",
    "                gwas_filename = f\"gwas_chr{chrom}{chrom_prefix}.{trait_id}.glm.{file_suffix}\"\n",
    "                eur_gwas_path = os.path.join(eur_base_dir, \"train\", \"gwas\", f\"fold_{group_num}\", gwas_filename)\n",
    "                if os.path.exists(eur_gwas_path):\n",
    "                    eur_output_path = os.path.join(output_gwas_dir, \"EUR\", trait, f\"group_{group_num}_chr{chrom}.txt\")\n",
    "                    if os.path.exists(eur_output_path):\n",
    "                        print(f\"Output file already exists, skipping: {eur_output_path}\")\n",
    "                        continue\n",
    "                    if not os.path.exists(os.path.dirname(eur_output_path)):\n",
    "                        os.makedirs(os.path.dirname(eur_output_path))\n",
    "                    get_summary_stats(trait, group_num, eur_gwas_path, eur_output_path, snp_list, n_control, n_case)\n",
    "                else:\n",
    "                    print(f\"GWAS file not found: {eur_gwas_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
