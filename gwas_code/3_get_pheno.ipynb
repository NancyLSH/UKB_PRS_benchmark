{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b765b556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5_get_pheno.py\n",
    "# This script merges multiple phenotype files from the UK Biobank into a single file.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "data_path_list = [\n",
    "    '/data1/jiapl_group/lishuhua/UKB/Phenotype/fields/core_category/fields_group_1.tsv',\n",
    "    '/data1/jiapl_group/lishuhua/UKB/Phenotype/fields/core_category/fields_group_2.tsv',\n",
    "    '/data1/jiapl_group/lishuhua/UKB/Phenotype/fields/core_category/fields_group_17.tsv',\n",
    "    '/data1/jiapl_group/lishuhua/UKB/Phenotype/fields/core_category/fields_group_37.tsv',\n",
    "    '/data1/jiapl_group/lishuhua/UKB/Phenotype/fields/core_category/fields_group_39.tsv',\n",
    "    '/data1/jiapl_group/lishuhua/UKB/Phenotype/fields/core_category/fields_group_73.tsv',\n",
    "    '/data1/jiapl_group/lishuhua/UKB/Phenotype/fields/core_category/fields_group_75.tsv',\n",
    "    '/data1/jiapl_group/lishuhua/UKB/Phenotype/fields/core_category/fields_group_76.tsv',\n",
    "    '/data1/jiapl_group/lishuhua/UKB/Phenotype/fields/core_category/fields_group_83.tsv',\n",
    "    '/data1/jiapl_group/lishuhua/UKB/Phenotype/fields/core_category/fields_group_84.tsv',\n",
    "    '/data1/jiapl_group/lishuhua/UKB/Phenotype/fields/core_category/fields_group_85.tsv',\n",
    "    '/data1/jiapl_group/lishuhua/UKB/Phenotype/fields/core_category/fields_group_86.tsv',\n",
    "    '/data1/jiapl_group/lishuhua/UKB/Phenotype/fields/core_category/fields_group_87.tsv',\n",
    "    '/data1/jiapl_group/lishuhua/UKB/Phenotype/fields/core_category/fields_group_88.tsv',\n",
    "    '/data1/jiapl_group/lishuhua/UKB/Phenotype/fields/core_category/fields_group_91.tsv',\n",
    "    '/data1/jiapl_group/lishuhua/UKB/Phenotype/fields/core_category/fields_group_92.tsv',\n",
    "]\n",
    "\n",
    "fields_ids = ['p48_i0', 'p48_i1', 'p48_i2', 'p48_i3', 'p50_i0', 'p50_i1', 'p50_i2', 'p50_i3', 'p102_i0_a0', 'p102_i0_a1', 'p102_i1_a0', 'p102_i1_a1', 'p102_i2_a0', 'p102_i2_a1', 'p102_i3_a0', 'p102_i3_a1', 'p4079_i0_a0', 'p4079_i0_a1', 'p4079_i1_a0', 'p4079_i1_a1', 'p4079_i2_a0', 'p4079_i2_a1', 'p4079_i3_a0', 'p4079_i3_a1', 'p4080_i0_a0', 'p4080_i0_a1', 'p4080_i1_a0', 'p4080_i1_a1', 'p4080_i2_a0', 'p4080_i2_a1', 'p4080_i3_a0', 'p4080_i3_a1', 'p20116_i0', 'p20116_i1', 'p20116_i2', 'p20116_i3', 'p20117_i0', 'p20117_i1', 'p20117_i2', 'p20117_i3', 'p21001_i0', 'p21001_i1', 'p21001_i2', 'p21001_i3', 'p21002_i0', 'p21002_i1', 'p21002_i2', 'p21002_i3', 'p30000_i0', 'p30000_i1', 'p30000_i2', 'p30010_i0', 'p30010_i1', 'p30010_i2', 'p30020_i0', 'p30020_i1', 'p30020_i2', 'p30080_i0', 'p30080_i1', 'p30080_i2', 'p30120_i0', 'p30120_i1', 'p30120_i2', 'p30130_i0', 'p30130_i1', 'p30130_i2', 'p30140_i0', 'p30140_i1', 'p30140_i2', 'p30150_i0', 'p30150_i1', 'p30150_i2',\n",
    "'p30620_i0', 'p30620_i1', 'p30650_i0', 'p30650_i1', 'p30670_i0', 'p30670_i1', 'p30690_i0', 'p30690_i1',\n",
    "'p30700_i0', 'p30700_i1', 'p30730_i0', 'p30730_i1', 'p30740_i0', 'p30740_i1', 'p30760_i0', 'p30760_i1',\n",
    "'p30780_i0', 'p30780_i1', 'p30870_i0', 'p30870_i1', 'p30880_i0', 'p30880_i1']\n",
    "\n",
    "data_frames = []\n",
    "for file_path in data_path_list:\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File {file_path} does not exist.\")\n",
    "        continue\n",
    "    columns = pd.read_csv(file_path, sep='\\t', nrows=0).columns.tolist()\n",
    "    overlap_columns = set(columns).intersection(fields_ids)\n",
    "    if not overlap_columns:\n",
    "        print(f\"No overlapping columns found in {file_path}.\")\n",
    "        continue\n",
    "    df = pd.read_csv(file_path, sep='\\t', usecols=['eid'] + list(overlap_columns), header=0, na_values=['NA', '.', '-9', ''])\n",
    "    data_frames.append(df)\n",
    "\n",
    "if data_frames:\n",
    "    merge_df = data_frames[0]\n",
    "    for df in data_frames[1:]:\n",
    "        merge_df = pd.merge(merge_df, df, on='eid', how='outer')\n",
    "    \n",
    "    output_file = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/UKB/pheno/trait/trait_ukb.tsv\"\n",
    "    merge_df.to_csv(output_file, sep='\\t', index=False, header=True, na_rep='')\n",
    "    print(f\"Data merged successfully and saved to {output_file}.\")\n",
    "else:\n",
    "    print(\"No data frames to merge. Please check the input files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ede92f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7_merge_ukb_pheno.py\n",
    "\n",
    "# merge all instances and arrays into a single column\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm, rankdata\n",
    "import os\n",
    "import re\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "def inverse_normal_transform(x):\n",
    "    \"\"\"Perform inverse normal transformation on a series.\"\"\"\n",
    "    values = np.array(x)\n",
    "    is_na = np.isnan(values)\n",
    "    ranks = rankdata(values[~is_na], method='average')\n",
    "    transformed = np.empty_like(values, dtype=float)\n",
    "    transformed[~is_na] = norm.ppf( (ranks - 0.5) / len(ranks) )\n",
    "    transformed[is_na] = np.nan  # Keep NaNs in the same places\n",
    "    # Ensure the transformed values are calulated correctly\n",
    "    transformed = pd.Series(transformed, index=x.index)\n",
    "    # transformed = transformed.astype(float)  # Ensure the type is float\n",
    "    return transformed\n",
    "\n",
    "def change_category_to_binary(x):\n",
    "    \"\"\"Convert a series to binary (0/1) if it contains only two unique values.\"\"\"\n",
    "    # change all -3 as NaN\n",
    "    x = x.replace(-3, np.nan)\n",
    "    # Only proceed if there are exactly two unique non-NaN values\n",
    "    unique_vals = pd.Series(x).dropna().unique()\n",
    "    if len(unique_vals) == 3:\n",
    "        # Map the smaller value to 0, the larger to 1\n",
    "        sorted_vals = sorted(unique_vals)\n",
    "        mapping = {sorted_vals[0]: 1, sorted_vals[1]: 1, sorted_vals[2]: 2}\n",
    "        x = x.map(mapping)\n",
    "    transformed = pd.Series(x, index=x.index)\n",
    "    return transformed\n",
    "\n",
    "def is_categorical(series, max_n_unique=20, min_n_unique=2):\n",
    "    \"\"\"Check if a series is categorical based on the number of unique values.\"\"\"\n",
    "    series = series.dropna()\n",
    "    unique_vals = set(series.unique())\n",
    "\n",
    "    if series.dtype.kind not in 'iui' and not all( (float(x).is_integer() for x in unique_vals) ):\n",
    "        return False\n",
    "    n_unique = len(unique_vals)\n",
    "    if 1 < n_unique <= max_n_unique and not unique_vals.issubset({0, 1}):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def process_one_phenotype(df, pheno_prefix, output_dir='/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/UKB/pheno/trait/'):\n",
    "    pattern = re.compile(rf\"^{pheno_prefix}_i\\d+(_a\\d+)?$\")\n",
    "    cols = [col for col in df.columns if pattern.match(col)]\n",
    "\n",
    "    if not cols:\n",
    "        print(f\"No columns found for prefix {pheno_prefix}.\")\n",
    "        return None\n",
    "    print(f\"[{pheno_prefix}] Processing columns: {cols}\")\n",
    "\n",
    "    pheno_raw = df[cols].bfill(axis=1).iloc[:, 0]\n",
    "\n",
    "    categorical = is_categorical(pheno_raw)\n",
    "\n",
    "    n_total = pheno_raw.shape[0]\n",
    "    n_no_missing = pheno_raw.notna().sum()\n",
    "\n",
    "    result = pd.DataFrame({\n",
    "        'eid': df['eid'],\n",
    "        f'{pheno_prefix}_raw': pheno_raw\n",
    "    })\n",
    "    \n",
    "    # figure_output_path = os.path.join(output_dir, f\"/distribution/\")\n",
    "    # os.makedirs(os.path.dirname(figure_output_path), exist_ok=True)\n",
    "\n",
    "    if categorical:\n",
    "        counts = pheno_raw.value_counts(dropna=True).to_dict()\n",
    "        pheno_int = change_category_to_binary(pheno_raw)\n",
    "        result[f'{pheno_prefix}_int'] = pheno_int\n",
    "        print(f\"[Categorical] Non-missing: {n_no_missing}, Category counts: {counts}\")\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.countplot(x=pheno_raw)\n",
    "        plt.title(f\"{pheno_prefix} Raw Distribution (Categorical)\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.countplot(x=pheno_int)\n",
    "        plt.title(f\"{pheno_prefix} Transform Distribution (Categorical)\")\n",
    "\n",
    "        plt.xlabel(\"Category\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/distribution/{pheno_prefix}_dist.png')\n",
    "        plt.close()\n",
    "    else:\n",
    "        print(f\"[Continuous] Non-missing: {n_no_missing}, Mean: {pheno_raw.mean()}, Std: {pheno_raw.std()}\")\n",
    "        pheno_int = pd.Series(inverse_normal_transform(pheno_raw), index=pheno_raw.index)\n",
    "        result[f'{pheno_prefix}_int'] = pheno_int\n",
    "\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.histplot(pheno_raw, kde=True, bins=50, color='skyblue')\n",
    "        plt.title(f\"{pheno_prefix} Raw Distribution\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.histplot(pheno_int, kde=True, bins=50, color='salmon')\n",
    "        plt.title(f\"{pheno_prefix} Inverse Normal Distribution\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/distribution/{pheno_prefix}_dist.png')\n",
    "        plt.close()\n",
    "    \n",
    "    return result\n",
    "\n",
    "def batch_process(df, pheno_prefixes, output_dir='/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/UKB/pheno/trait/'):\n",
    "    all_results = []\n",
    "    \n",
    "    for prefix in pheno_prefixes:\n",
    "        result = process_one_phenotype(df, prefix, output_dir)\n",
    "        if result is not None:\n",
    "            all_results.append(result)\n",
    "    if all_results:\n",
    "        all_merged = all_results[0]\n",
    "        for res in all_results[1:]:\n",
    "            all_merged = pd.merge(all_merged, res, on='eid', how='outer')\n",
    "        output_file = os.path.join(output_dir, 'trait_ukb_processed.tsv')\n",
    "        all_merged.to_csv(output_file, sep='\\t', index=False, header=True, na_rep='')\n",
    "        print(f\"Processed data saved to {output_file}.\")\n",
    "    else:\n",
    "        print(\"No valid phenotypes processed. Please check the input data.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trait_path = '/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/UKB/pheno/trait/trait_ukb.tsv'\n",
    "    trait_df = pd.read_csv(trait_path, sep='\\t', na_values=['NA', '.', '-9', ''])\n",
    "    \n",
    "    pheno_prefixes = [\n",
    "        'p48', 'p50', 'p102', 'p4079', 'p4080', 'p20116', 'p20117', 'p21001', 'p21002',\n",
    "        'p30000', 'p30010', 'p30020', 'p30080', 'p30120', 'p30130', 'p30140',\n",
    "        'p30150', 'p30620', 'p30650', 'p30670', 'p30690', 'p30700',\n",
    "        'p30730', 'p30740', 'p30760', 'p30780', 'p30870', 'p30880']\n",
    "\n",
    "    batch_process(trait_df, pheno_prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad90bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## QC trait data and split into EUR and EAS\n",
    "import pandas as pd\n",
    "\n",
    "white_british_list_path = '/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/UKB/pheno/covar/covars_white_british_qced.txt'\n",
    "chinese_list_path = '/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/UKB/pheno/covar/covars_chinese_qced.txt'\n",
    "trait_path = '/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/UKB/pheno/trait/trait_ukb_processed.tsv'\n",
    "\n",
    "white_british_list = pd.read_csv(white_british_list_path, sep='\\t', header=None, names=['FID', 'IID'])\n",
    "chinese_list = pd.read_csv(chinese_list_path, sep='\\t', header=None, names=['FID', 'IID'])\n",
    "\n",
    "trait_df = pd.read_csv(trait_path, sep='\\t', na_values=['NA', '.', '-9', ''])\n",
    "trait_df['FID'] = trait_df['eid']\n",
    "trait_df['IID'] = trait_df['eid']\n",
    "\n",
    "# Merge with white British and Chinese lists\n",
    "trait_white_british = pd.merge(white_british_list, trait_df, on=['FID', 'IID'], how='inner')\n",
    "trait_chinese = pd.merge(chinese_list, trait_df, on=['FID', 'IID'], how='inner')\n",
    "\n",
    "id_cols = ['FID', 'IID']\n",
    "trait_cols_eur = [col for col in trait_white_british.columns if col not in ['FID', 'IID', 'eid']]\n",
    "trait_cols_eas = [col for col in trait_chinese.columns if col not in ['FID', 'IID', 'eid']]\n",
    "for trait in trait_cols_eur:\n",
    "    df_trait = trait_white_british[ id_cols + [trait] ].copy()\n",
    "    df_trait = df_trait.dropna(subset=[trait])\n",
    "    output_path = f'/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/UKB/pheno/trait/White_British/{trait}.txt'\n",
    "    df_trait.to_csv(output_path, sep='\\t', index=False, header=True, na_rep='', quoting=3, encoding='utf-8')\n",
    "    print(f\"Trait {trait} for White British saved to {output_path}, {df_trait.shape[0]} samples.\")\n",
    "\n",
    "for trait in trait_cols_eas:\n",
    "    df_trait = trait_chinese[id_cols + [trait]].copy()\n",
    "    df_trait = df_trait.dropna(subset=[trait])\n",
    "    output_path = f'/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/UKB/pheno/trait/Chinese/{trait}.txt'\n",
    "    df_trait.to_csv(output_path, sep='\\t', index=False, header=True, na_rep='', quoting=3, encoding='utf-8')\n",
    "    print(f\"Trait {trait} for Chinese saved to {output_path}, {df_trait.shape[0]} samples.\")\n",
    "\n",
    "# check the number of samples\n",
    "print(f\"Number of white British samples: {trait_white_british.shape[0]}\")\n",
    "print(f\"Number of Chinese samples: {trait_chinese.shape[0]}\")\n",
    "\n",
    "# Save the results\n",
    "trait_white_british.to_csv('/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/UKB/pheno/trait/trait_ukb_white_british.txt', sep='\\t', index=False, header=True, na_rep='', quoting=3, encoding='utf-8')\n",
    "trait_chinese.to_csv('/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/UKB/pheno/trait/trait_ukb_chinese.txt', sep='\\t', index=False, header=True, na_rep='', quoting=3, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a115e053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7_merge_ukb_pheno.py\n",
    "\n",
    "# merge all instances and arrays into a single column\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm, rankdata\n",
    "import os\n",
    "import re\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "def inverse_normal_transform(x):\n",
    "    \"\"\"Perform inverse normal transformation on a series.\"\"\"\n",
    "    values = np.array(x)\n",
    "    is_na = np.isnan(values)\n",
    "    ranks = rankdata(values[~is_na], method='average')\n",
    "    transformed = np.empty_like(values, dtype=float)\n",
    "    transformed[~is_na] = norm.ppf( (ranks - 0.5) / len(ranks) )\n",
    "    transformed[is_na] = np.nan  # Keep NaNs in the same places\n",
    "    # Ensure the transformed values are calulated correctly\n",
    "    transformed = pd.Series(transformed, index=x.index)\n",
    "    # transformed = transformed.astype(float)  # Ensure the type is float\n",
    "    return transformed\n",
    "\n",
    "def change_category_to_binary(x):\n",
    "    \"\"\"Convert a series to binary (0/1) if it contains only two unique values.\"\"\"\n",
    "    # change all -3 as NaN\n",
    "    x = x.replace(-3, np.nan)\n",
    "    # Only proceed if there are exactly two unique non-NaN values\n",
    "    unique_vals = pd.Series(x).dropna().unique()\n",
    "    if len(unique_vals) == 3:\n",
    "        # Map the smaller value to 0, the larger to 1\n",
    "        sorted_vals = sorted(unique_vals)\n",
    "        mapping = {sorted_vals[0]: 1, sorted_vals[1]: 1, sorted_vals[2]: 2}\n",
    "        x = x.map(mapping)\n",
    "    transformed = pd.Series(x, index=x.index)\n",
    "    return transformed\n",
    "\n",
    "def is_categorical(series, max_n_unique=20, min_n_unique=2):\n",
    "    \"\"\"Check if a series is categorical based on the number of unique values.\"\"\"\n",
    "    series = series.dropna()\n",
    "    unique_vals = set(series.unique())\n",
    "\n",
    "    if series.dtype.kind not in 'iui' and not all( (float(x).is_integer() for x in unique_vals) ):\n",
    "        return False\n",
    "    n_unique = len(unique_vals)\n",
    "    if 1 < n_unique <= max_n_unique and not unique_vals.issubset({0, 1}):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def process_one_phenotype(df, pheno_prefix, output_dir):\n",
    "\n",
    "    pheno_data = df[['eid'] + [col for col in df.columns if col.startswith(pheno_prefix)]]\n",
    "    print(f\"[{pheno_prefix}] Processing columns: {pheno_data.columns.tolist()}\")\n",
    "\n",
    "    categorical = is_categorical(pheno_data[[col for col in pheno_data.columns if col.endswith(\"_raw\")]].iloc[:, 0])\n",
    "    raw_data = pheno_data[[col for col in pheno_data.columns if col.endswith(\"_raw\")]].iloc[:, 0]\n",
    "\n",
    "    # figure_output_path = os.path.join(output_dir, f\"/distribution/\")\n",
    "    # os.makedirs(os.path.dirname(figure_output_path), exist_ok=True)\n",
    "\n",
    "    if categorical:\n",
    "        counts = raw_data.value_counts(dropna=True).to_dict()\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        # plt.subplot(1, 2, 1)\n",
    "        sns.countplot(x=raw_data)\n",
    "        plt.title(f\"{pheno_prefix} Raw Distribution (Categorical)\")\n",
    "\n",
    "        # plt.subplot(1, 2, 2)\n",
    "        # sns.countplot(x=int_data)\n",
    "        # plt.title(f\"{int_data} Transform Distribution (Categorical)\")\n",
    "\n",
    "        plt.xlabel(\"Category\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/{pheno_prefix}_dist.png')\n",
    "        plt.close()\n",
    "    else:\n",
    "        int_data = pheno_data[[col for col in pheno_data.columns if col.endswith(\"_int\")]].iloc[:, 0]\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.histplot(raw_data, kde=True, bins=50, color='skyblue')\n",
    "        plt.title(f\"{pheno_prefix} Raw Distribution\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.histplot(int_data, kde=True, bins=50, color='salmon')\n",
    "        plt.title(f\"{pheno_prefix} Inverse Normal Distribution\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/{pheno_prefix}_dist.png')\n",
    "        plt.close()\n",
    "\n",
    "def batch_process(df, pheno_prefixes, output_dir='/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/UKB/pheno/trait/'):\n",
    "    \n",
    "    for prefix in pheno_prefixes:\n",
    "        process_one_phenotype(df, prefix, output_dir)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # trait_path = '/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/UKB/pheno/trait/trait_ukb.tsv'\n",
    "    chinese_df = pd.read_csv('/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/UKB/pheno/trait/trait_ukb_chinese.txt', sep='\\t', na_values=['NA', '.', '-9', ''])\n",
    "    white_british_df = pd.read_csv('/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/UKB/pheno/trait/trait_ukb_white_british.txt', sep='\\t', na_values=['NA', '.', '-9', ''])\n",
    "    # trait_df = pd.read_csv(trait_path, sep='\\t', na_values=['NA', '.', '-9', ''])\n",
    "    \n",
    "    pheno_prefixes = [\n",
    "        'p48', 'p50', 'p102', 'p4079', 'p4080', 'p20116', 'p20117', 'p21001', 'p21002',\n",
    "        'p30000', 'p30010', 'p30020', 'p30080', 'p30120', 'p30130', 'p30140',\n",
    "        'p30150', 'p30620', 'p30650', 'p30670', 'p30690', 'p30700',\n",
    "        'p30730', 'p30740', 'p30760', 'p30780', 'p30870', 'p30880']\n",
    "\n",
    "    batch_process(chinese_df, pheno_prefixes, '/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/UKB/pheno/trait/Chinese_distribution/')\n",
    "    batch_process(white_british_df, pheno_prefixes, '/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/UKB/pheno/trait/White_British_distribution/')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
