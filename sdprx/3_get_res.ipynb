{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0aa094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "beta_file_path = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/software/sdprx/res/data/\"\n",
    "output_path = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/software/sdprx/res/merged_data/\"\n",
    "\n",
    "# alt_p30620_chr10_2.txt\n",
    "\n",
    "def merge_beta_files(trait):\n",
    "    file_pattern = f\"{trait}_chr*_2.txt\"\n",
    "    output_file_path = f\"{output_path}{trait}_merged.txt\"\n",
    "    search_path = f\"{beta_file_path}{file_pattern}\"\n",
    "    file_list = sorted(glob.glob(search_path))\n",
    "    if not file_list:\n",
    "        print(f\"No files found for trait: {trait}\")\n",
    "        return\n",
    "    else:\n",
    "        print(f\"Found {len(file_list)} files for trait: {trait}\")\n",
    "        df_list = []\n",
    "        for f in file_list:\n",
    "            print(f\"Processing file: {f}\")\n",
    "            df = pd.read_csv(f, sep=r'\\s+', header=0)\n",
    "            df_list.append(df)\n",
    "        merged_df = pd.concat(df_list, ignore_index=True)\n",
    "        merged_df.to_csv(output_file_path, sep='\\t', header=False, index=False)\n",
    "\n",
    "trait_dict = {\n",
    "    'p48': 'waist',\n",
    "    'p50': 'height',\n",
    "    'p102': 'pulse',\n",
    "    'p4079': 'dbp',\n",
    "    'p4080': 'sbp',\n",
    "    'p20116': 'smoke',\n",
    "    'p20117': 'drink',\n",
    "    'p21001': 'bmi',\n",
    "    'p21002': 'weight',\n",
    "    'p30000': 'wbc',\n",
    "    'p30010': 'rbc',\n",
    "    'p30020':'hb',\n",
    "    'p30080': 'plt',\n",
    "    'p30120': 'lymph',\n",
    "    'p30130': 'mono',\n",
    "    'p30140': 'neut',\n",
    "    'p30150': 'eos',\n",
    "    'p30620': 'alt',\n",
    "    'p30650': 'ast',\n",
    "    'p30670': 'bun',\n",
    "    'p30690': 'cholesterol',\n",
    "    'p30700': 'creatinine',\n",
    "    'p30730': 'ggt',\n",
    "    'p30740': 'glucose',\n",
    "    'p30760': 'hdl',\n",
    "    'p30780': 'ldl',\n",
    "    'p30870': 'triglycerides',\n",
    "    'p30880': 'ua'\n",
    "}\n",
    "\n",
    "for trait, name in trait_dict.items():\n",
    "    trait_name = f'{name}_{trait}'\n",
    "    print(f\"Merging files for trait: {trait_name}\")\n",
    "    merge_beta_files(trait_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551463d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "beta_file_path = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/software/sdprx/res/merged_data/\"\n",
    "test_bfile_path = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/UKB/geno/Chinese/1_merged/merged\"\n",
    "plink_path = \"/data1/jiapl_group/lishuhua/software/general/plink\"\n",
    "output_path = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/software/sdprx/res/test_prs\"\n",
    "\n",
    "tasks = []\n",
    "# alt_p30620_EAS_pst_eff_a1_b0.5_phiauto_chr15.txt\n",
    "for beta_file in os.listdir(beta_file_path):\n",
    "    if beta_file.endswith(\".txt\"):\n",
    "        beta = os.path.join(beta_file_path, beta_file)\n",
    "        trait = beta_file.replace(\"_merged.txt\", \"\")\n",
    "        output = os.path.join(output_path, f\"{trait}_PRS\")\n",
    "        cmd = f\"{plink_path} --bfile {test_bfile_path} --score {beta} 1 2 3 no-mean-imputation --out {output}\"\n",
    "        tasks.append(cmd)\n",
    "\n",
    "pool = Pool(cpu_count()-1)\n",
    "pool.map(os.system, tasks)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ec01e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "prs_file_path = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/software/sdprx/res/test_prs\"\n",
    "output_path = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/software/sdprx/res/test_prs_cleaned\"\n",
    "\n",
    "for prs_file in os.listdir(prs_file_path):\n",
    "    if prs_file.endswith(\"PRS.profile\"):\n",
    "        prs_data = open(os.path.join(prs_file_path, prs_file), \"r\")\n",
    "        prs_list = []\n",
    "        for line in prs_data:\n",
    "            line_arr = line.strip().split(\" \")\n",
    "            line_arr = [x for x in line_arr if x != \"\"]\n",
    "            prs_list.append(line_arr)\n",
    "        prs_data.close()\n",
    "        prs_df = pd.DataFrame(prs_list[1:], columns=prs_list[0])\n",
    "        trait = prs_file.replace(\"_PRS.profile\", \"\")\n",
    "        prs_df.to_csv(os.path.join(output_path, f\"{trait}_PRS.tsv\"), sep='\\t', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd26c287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, mean_squared_error\n",
    "from sklearn.calibration import calibration_curve\n",
    "from scipy.stats import pearsonr\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# 忽略statsmodels在某些拟合中可能产生的警告\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "warnings.simplefilter('ignore', ConvergenceWarning)\n",
    "\n",
    "# --- 1. 指标计算函数 (从原脚本逻辑封装而来) ---\n",
    "\n",
    "def calculate_continuous_metrics(df, base_covars, full_covars):\n",
    "    \"\"\"为给定的数据集(df)计算连续性状的所有性能指标。\"\"\"\n",
    "    # 增量R²\n",
    "    model_base = sm.OLS(df[\"trait\"], sm.add_constant(df[base_covars])).fit()\n",
    "    model_full = sm.OLS(df[\"trait\"], sm.add_constant(df[full_covars])).fit()\n",
    "    r2_incremental = model_full.rsquared - model_base.rsquared\n",
    "\n",
    "    # 皮尔逊相关系数 (SCORE vs. 表型残差)\n",
    "    pheno_residuals = model_base.resid\n",
    "    corr, _ = pearsonr(df[\"SCORE\"], pheno_residuals)\n",
    "\n",
    "    # RMSE\n",
    "    prediction_full = model_full.predict(sm.add_constant(df[full_covars]))\n",
    "    rmse = np.sqrt(mean_squared_error(df[\"trait\"], prediction_full))\n",
    "    nrmse_mean = rmse / df[\"trait\"].mean() if df[\"trait\"].mean() != 0 else np.nan\n",
    "    nrmse_range = rmse / (df[\"trait\"].max() - df[\"trait\"].min()) if (df[\"trait\"].max() - df[\"trait\"].min()) != 0 else np.nan\n",
    "    nrmse_std = rmse / df[\"trait\"].std() if df[\"trait\"].std() != 0 else np.nan\n",
    "\n",
    "    # 分位数均值\n",
    "    df['quantile'] = pd.qcut(df['SCORE'], 5, labels=False, duplicates='drop')\n",
    "    quantile_means = df.groupby('quantile')['trait'].mean()\n",
    "    \n",
    "    return {\n",
    "        \"r2_incremental\": r2_incremental,\n",
    "        \"r2_full\": model_full.rsquared,\n",
    "        \"rmse\": rmse,\n",
    "        \"nrmse_mean\": nrmse_mean,\n",
    "        \"nrmse_range\": nrmse_range,\n",
    "        \"nrmse_std\": nrmse_std,\n",
    "        \"pearson_r\": corr,\n",
    "        \"top_quintile_mean\": quantile_means.iloc[-1],\n",
    "        \"bottom_quintile_mean\": quantile_means.iloc[0]\n",
    "    }\n",
    "\n",
    "def calculate_binary_metrics(df, base_covars, full_covars):\n",
    "    \"\"\"为给定的数据集(df)计算二元性状的所有性能指标。\"\"\"\n",
    "    # AUC 和 PR-AUC\n",
    "    logit_model = sm.Logit(df[\"trait\"], sm.add_constant(df[full_covars])).fit(disp=0)\n",
    "    pred_prob = logit_model.predict(sm.add_constant(df[full_covars]))\n",
    "    auc = roc_auc_score(df[\"trait\"], pred_prob)\n",
    "    pr_auc = average_precision_score(df[\"trait\"], pred_prob)\n",
    "\n",
    "    # 每1-SD的OR\n",
    "    df[\"prs_scaled\"] = (df[\"SCORE\"] - df[\"SCORE\"].mean()) / df[\"SCORE\"].std()\n",
    "    logit_model_scaled = sm.Logit(df[\"trait\"], sm.add_constant(df[base_covars + [\"prs_scaled\"]])).fit(disp=0)\n",
    "    or_per_sd = np.exp(logit_model_scaled.params[\"prs_scaled\"])\n",
    "\n",
    "    # 分位数OR\n",
    "    df['prs_quintile'] = pd.qcut(df['SCORE'], 5, labels=False, duplicates='drop')\n",
    "    reference_quintile = 2\n",
    "    or_quintiles = {}\n",
    "    for q in range(5):\n",
    "        if q == reference_quintile:\n",
    "            or_quintiles[f'OR_Quintile_{q+1}'] = 1.0\n",
    "            continue\n",
    "        temp_df = df[df['prs_quintile'].isin([q, reference_quintile])].copy()\n",
    "        temp_df['is_current_quintile'] = (temp_df['prs_quintile'] == q).astype(int)\n",
    "        X_quintile = sm.add_constant(temp_df[['is_current_quintile'] + base_covars])\n",
    "        model_q = sm.Logit(temp_df[\"trait\"], X_quintile).fit(disp=0)\n",
    "        or_quintiles[f'OR_Quintile_{q+1}'] = np.exp(model_q.params['is_current_quintile'])\n",
    "        \n",
    "    results = {\n",
    "        \"auc\": auc,\n",
    "        \"pr_auc\": pr_auc,\n",
    "        \"or_per_sd\": or_per_sd,\n",
    "    }\n",
    "    results.update(or_quintiles) # 将分位数OR合并到结果字典\n",
    "    return results\n",
    "\n",
    "# --- 2. Bootstrap核心分析函数 ---\n",
    "\n",
    "def bootstrap_analysis(df, n_bootstrap, analysis_func, base_covars, full_covars):\n",
    "    \"\"\"\n",
    "    对给定的数据集执行Bootstrap分析。\n",
    "    \n",
    "    参数:\n",
    "    - df: 完整的数据集 (DataFrame)。\n",
    "    - n_bootstrap: Bootstrap重复次数。\n",
    "    - analysis_func: 用于在每个样本上计算指标的函数 (例如, calculate_continuous_metrics)。\n",
    "    - base_covars, full_covars: 协变量列表。\n",
    "\n",
    "    返回:\n",
    "    - 一个包含点估计和95%置信区间的字典。\n",
    "    \"\"\"\n",
    "    bootstrap_results = []\n",
    "    for i in range(n_bootstrap):\n",
    "        # 创建自助样本 (有放回抽样)\n",
    "        sample_df = df.sample(n=len(df), replace=True)\n",
    "        \n",
    "        try:\n",
    "            # 在自助样本上计算指标\n",
    "            metrics = analysis_func(sample_df, base_covars, full_covars)\n",
    "            bootstrap_results.append(metrics)\n",
    "        except Exception as e:\n",
    "            # 在某些自助样本中，由于数据特殊性(如二元性状只有一类)，模型可能无法拟合\n",
    "            # 此时可以跳过这次失败的抽样\n",
    "            print(f\"Bootstrap iteration {i+1} failed with error: {e}. Skipping.\")\n",
    "            continue\n",
    "    \n",
    "    # 将结果列表转换为DataFrame\n",
    "    results_df = pd.DataFrame(bootstrap_results)\n",
    "    \n",
    "    # 计算点估计 (中位数) 和 95% CI\n",
    "    final_report = {}\n",
    "    for col in results_df.columns:\n",
    "        point_estimate = results_df[col].median()\n",
    "        ci_lower = results_df[col].quantile(0.025)\n",
    "        ci_upper = results_df[col].quantile(0.975)\n",
    "        final_report[f'{col}_median'] = point_estimate\n",
    "        final_report[f'{col}_CI_lower'] = ci_lower\n",
    "        final_report[f'{col}_CI_upper'] = ci_upper\n",
    "        \n",
    "    return final_report\n",
    "\n",
    "# --- 3. 主执行流程 ---\n",
    "\n",
    "def main():\n",
    "    # --- 参数设置 ---\n",
    "    # !! 注意: 请根据您的实际路径修改下面的变量 !!\n",
    "    cleaned_prs_path = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/software/sdprx/res/test_prs_cleaned\"\n",
    "    covar_path = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/UKB/pheno/covar/covars_chinese_final.tsv\"\n",
    "    output_dir = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/software/sdprx/res/full_res\"\n",
    "    pheno_dir = \"/data1/jiapl_group/lishuhua/project/PRS_benchmark/real_data/UKB/pheno/trait/Chinese\"\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    n_bootstrap = 1000 # 推荐值, 可设为100进行快速测试\n",
    "    \n",
    "    # --- 初始化 ---\n",
    "    final_results_continuous = []\n",
    "    final_results_binary = []\n",
    "    covar_cols = [\"FID\", \"IID\", \"age\", \"sex\"] + [f\"PC{i}\" for i in range(1, 11)]\n",
    "    base_covars = [\"age\", \"sex\"] + [f\"PC{i}\" for i in range(1, 11)]\n",
    "    full_covars = base_covars + [\"SCORE\"]\n",
    "\n",
    "    # --- 数据加载与处理 ---\n",
    "    covars = pd.read_csv(covar_path, sep='\\t', usecols=covar_cols)\n",
    "    \n",
    "    for prs_file in os.listdir(cleaned_prs_path):\n",
    "        trait_id_from_prs = prs_file.split(\"_\")[1]\n",
    "        if trait_id_from_prs == \"p20116\" or trait_id_from_prs == \"p20117\":\n",
    "            trait = f\"{trait_id_from_prs}_int\"\n",
    "        else:\n",
    "            trait = f\"{trait_id_from_prs}_raw\"\n",
    "        \n",
    "        # 寻找对应的表型文件\n",
    "        pheno_file_found = None\n",
    "        for pheno_file in os.listdir(pheno_dir):\n",
    "            if pheno_file.startswith(trait) and pheno_file.endswith(\".txt\"):\n",
    "                pheno_file_found = pheno_file\n",
    "                break\n",
    "        \n",
    "        if not pheno_file_found:\n",
    "            print(f\"Warning: No phenotype file found for PRS trait {trait_id_from_prs}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nProcessing Trait: {trait_id_from_prs}\")\n",
    "        pheno = pd.read_csv(os.path.join(pheno_dir, pheno_file_found), sep='\\t', header=None)\n",
    "        pheno.columns = [\"FID\", \"IID\", \"trait\"]\n",
    "        prs = pd.read_csv(os.path.join(cleaned_prs_path, prs_file), sep='\\t')\n",
    "        \n",
    "        # 合并数据\n",
    "        # make sure FID and IID are in the same format\n",
    "        pheno[\"FID\"] = pheno[\"FID\"].astype(str)\n",
    "        pheno[\"IID\"] = pheno[\"IID\"].astype(str)\n",
    "        prs[\"FID\"] = prs[\"FID\"].astype(str)\n",
    "        prs[\"IID\"] = prs[\"IID\"].astype(str)\n",
    "        covars[\"FID\"] = covars[\"FID\"].astype(str)\n",
    "        covars[\"IID\"] = covars[\"IID\"].astype(str)\n",
    "        merged_data = pd.merge(pheno, prs, on=[\"FID\", \"IID\"], how=\"inner\")\n",
    "        merged_data = pd.merge(merged_data, covars, on=[\"FID\", \"IID\"], how=\"inner\")\n",
    "\n",
    "        # --- NEW: Add a defensive data cleaning and type conversion step ---\n",
    "        # 1. Define all columns that MUST be numeric for the analysis\n",
    "        numeric_cols = [\"trait\", \"SCORE\", \"age\", \"sex\"] + [f\"PC{i}\" for i in range(1, 11)]\n",
    "\n",
    "        # 2. Loop through the columns and force them to be numeric.\n",
    "        # The 'coerce' option is key: it will turn any problematic non-numeric\n",
    "        # value (e.g., a string 'NA') into a proper NaN.\n",
    "        for col in numeric_cols:\n",
    "            if col in merged_data.columns:\n",
    "                merged_data[col] = pd.to_numeric(merged_data[col], errors='coerce')\n",
    "\n",
    "        # 3. Now, drop any rows that contain NaN values (either original or newly created).\n",
    "        original_rows = len(merged_data)\n",
    "        merged_data.dropna(inplace=True)\n",
    "        new_rows = len(merged_data)\n",
    "\n",
    "        # 4. Optional: Print a warning if rows were dropped, so you are aware of data quality issues.\n",
    "        if original_rows > new_rows:\n",
    "            print(f\"--> Warning: Dropped {original_rows - new_rows} rows due to non-numeric data or NaNs.\")\n",
    "\n",
    "        print(f\"Data merged and cleaned for trait {trait_id_from_prs}. Total samples: {len(merged_data)}\")\n",
    "\n",
    "        # --- 执行分析 ---\n",
    "        if trait == \"p20116_int\" or trait == \"p20117_int\":\n",
    "            # 二元性状分析\n",
    "            # 确保二元性状是0/1编码\n",
    "            unique_vals = sorted(merged_data[\"trait\"].unique())\n",
    "            if set(unique_vals).issubset({0, 1}):\n",
    "                pass # 已经是0/1\n",
    "            elif len(unique_vals) == 2:\n",
    "                print(f\"Converting binary trait from {unique_vals} to 0/1.\")\n",
    "                merged_data[\"trait\"] = (merged_data[\"trait\"] == unique_vals[1]).astype(int)\n",
    "            else:\n",
    "                print(f\"Error: Binary trait column for {trait_id_from_prs} contains unexpected values: {unique_vals}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            analysis_report = bootstrap_analysis(merged_data, n_bootstrap, calculate_binary_metrics, base_covars, full_covars)\n",
    "            analysis_report['trait'] = trait_id_from_prs\n",
    "            final_results_binary.append(analysis_report)\n",
    "        else:\n",
    "            # 连续性状分析\n",
    "            analysis_report = bootstrap_analysis(merged_data, n_bootstrap, calculate_continuous_metrics, base_covars, full_covars)\n",
    "            analysis_report['trait'] = trait_id_from_prs\n",
    "            final_results_continuous.append(analysis_report)\n",
    "\n",
    "    # --- 4. 保存最终结果 ---\n",
    "    if final_results_continuous:\n",
    "        continuous_df = pd.DataFrame(final_results_continuous)\n",
    "        continuous_df.to_csv(os.path.join(output_dir, \"prs_continuous_metrics_with_ci.csv\"), index=False)\n",
    "        print(\"\\nContinuous trait results saved to prs_continuous_metrics_with_ci.csv\")\n",
    "        print(continuous_df)\n",
    "\n",
    "    if final_results_binary:\n",
    "        binary_df = pd.DataFrame(final_results_binary)\n",
    "        binary_df.to_csv(os.path.join(output_dir, \"prs_binary_metrics_with_ci.csv\"), index=False)\n",
    "        print(\"\\nBinary trait results saved to prs_binary_metrics_with_ci.csv\")\n",
    "        print(binary_df)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
